{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: absl-py==2.1.0 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 1)) (2.1.0)\n",
            "Requirement already satisfied: asttokens==2.4.1 in c:\\users\\vitor\\appdata\\roaming\\python\\python311\\site-packages (from -r .\\requirements.txt (line 2)) (2.4.1)\n",
            "Requirement already satisfied: astunparse==1.6.3 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 3)) (1.6.3)\n",
            "Requirement already satisfied: bleach==6.1.0 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 4)) (6.1.0)\n",
            "Requirement already satisfied: certifi==2024.2.2 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 5)) (2024.2.2)\n",
            "Requirement already satisfied: charset-normalizer==3.3.2 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 6)) (3.3.2)\n",
            "Requirement already satisfied: click==8.1.7 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 7)) (8.1.7)\n",
            "Requirement already satisfied: colorama==0.4.6 in c:\\users\\vitor\\appdata\\roaming\\python\\python311\\site-packages (from -r .\\requirements.txt (line 8)) (0.4.6)\n",
            "Requirement already satisfied: comm==0.2.2 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 9)) (0.2.2)\n",
            "Requirement already satisfied: contourpy==1.2.1 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 10)) (1.2.1)\n",
            "Requirement already satisfied: cycler==0.12.1 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 11)) (0.12.1)\n",
            "Requirement already satisfied: deap==1.4.1 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 12)) (1.4.1)\n",
            "Requirement already satisfied: debugpy==1.8.1 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 13)) (1.8.1)\n",
            "Requirement already satisfied: decorator==5.1.1 in c:\\users\\vitor\\appdata\\roaming\\python\\python311\\site-packages (from -r .\\requirements.txt (line 14)) (5.1.1)\n",
            "Requirement already satisfied: executing==2.0.1 in c:\\users\\vitor\\appdata\\roaming\\python\\python311\\site-packages (from -r .\\requirements.txt (line 15)) (2.0.1)\n",
            "Requirement already satisfied: filelock==3.14.0 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 16)) (3.14.0)\n",
            "Requirement already satisfied: flatbuffers==24.3.25 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 17)) (24.3.25)\n",
            "Requirement already satisfied: fonttools==4.53.0 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 18)) (4.53.0)\n",
            "Requirement already satisfied: fsspec==2024.5.0 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 19)) (2024.5.0)\n",
            "Requirement already satisfied: gast==0.5.4 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 20)) (0.5.4)\n",
            "Requirement already satisfied: gensim==4.3.2 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 21)) (4.3.2)\n",
            "Requirement already satisfied: google-pasta==0.2.0 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 22)) (0.2.0)\n",
            "Requirement already satisfied: grpcio==1.64.0 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 23)) (1.64.0)\n",
            "Requirement already satisfied: h5py==3.11.0 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 24)) (3.11.0)\n",
            "Requirement already satisfied: huggingface-hub==0.23.2 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 25)) (0.23.2)\n",
            "Requirement already satisfied: idna==3.7 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 26)) (3.7)\n",
            "Requirement already satisfied: imbalanced-learn==0.12.3 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 27)) (0.12.3)\n",
            "Requirement already satisfied: imblearn==0.0 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 28)) (0.0)\n",
            "Requirement already satisfied: intel-openmp==2021.4.0 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 29)) (2021.4.0)\n",
            "Requirement already satisfied: ipython==8.24.0 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 30)) (8.24.0)\n",
            "Requirement already satisfied: jedi==0.19.1 in c:\\users\\vitor\\appdata\\roaming\\python\\python311\\site-packages (from -r .\\requirements.txt (line 31)) (0.19.1)\n",
            "Requirement already satisfied: Jinja2==3.1.4 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 32)) (3.1.4)\n",
            "Requirement already satisfied: joblib==1.4.2 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 33)) (1.4.2)\n",
            "Requirement already satisfied: jupyter_client==8.6.2 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 34)) (8.6.2)\n",
            "Requirement already satisfied: jupyter_core==5.7.2 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 35)) (5.7.2)\n",
            "Requirement already satisfied: kaggle==1.6.14 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 36)) (1.6.14)\n",
            "Requirement already satisfied: keras==3.3.3 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 37)) (3.3.3)\n",
            "Requirement already satisfied: keras-tuner==1.4.7 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 38)) (1.4.7)\n",
            "Requirement already satisfied: kiwisolver==1.4.5 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 39)) (1.4.5)\n",
            "Requirement already satisfied: kt-legacy==1.0.5 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 40)) (1.0.5)\n",
            "Requirement already satisfied: libclang==18.1.1 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 41)) (18.1.1)\n",
            "Requirement already satisfied: Markdown==3.6 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 42)) (3.6)\n",
            "Requirement already satisfied: markdown-it-py==3.0.0 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 43)) (3.0.0)\n",
            "Requirement already satisfied: MarkupSafe==2.1.5 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 44)) (2.1.5)\n",
            "Requirement already satisfied: matplotlib==3.9.0 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 45)) (3.9.0)\n",
            "Requirement already satisfied: matplotlib-inline==0.1.7 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 46)) (0.1.7)\n",
            "Requirement already satisfied: mdurl==0.1.2 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 47)) (0.1.2)\n",
            "Requirement already satisfied: mkl==2021.4.0 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 48)) (2021.4.0)\n",
            "Requirement already satisfied: ml-dtypes==0.3.2 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 49)) (0.3.2)\n",
            "Requirement already satisfied: mpmath==1.3.0 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 50)) (1.3.0)\n",
            "Requirement already satisfied: namex==0.0.8 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 51)) (0.0.8)\n",
            "Requirement already satisfied: nest-asyncio==1.6.0 in c:\\users\\vitor\\appdata\\roaming\\python\\python311\\site-packages (from -r .\\requirements.txt (line 52)) (1.6.0)\n",
            "Requirement already satisfied: networkx==3.3 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 53)) (3.3)\n",
            "Requirement already satisfied: nltk==3.8.1 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 54)) (3.8.1)\n",
            "Requirement already satisfied: numpy==1.26.4 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 55)) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum==3.3.0 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 56)) (3.3.0)\n",
            "Requirement already satisfied: optree==0.11.0 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 57)) (0.11.0)\n",
            "Requirement already satisfied: packaging==24.0 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 58)) (24.0)\n",
            "Requirement already satisfied: pandas==2.2.2 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 59)) (2.2.2)\n",
            "Requirement already satisfied: parso==0.8.4 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 60)) (0.8.4)\n",
            "Requirement already satisfied: pillow==10.3.0 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 61)) (10.3.0)\n",
            "Requirement already satisfied: platformdirs==4.2.2 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 62)) (4.2.2)\n",
            "Requirement already satisfied: prompt-toolkit==3.0.43 in c:\\users\\vitor\\appdata\\roaming\\python\\python311\\site-packages (from -r .\\requirements.txt (line 63)) (3.0.43)\n",
            "Requirement already satisfied: protobuf==4.25.3 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 64)) (4.25.3)\n",
            "Requirement already satisfied: psutil==5.9.8 in c:\\users\\vitor\\appdata\\roaming\\python\\python311\\site-packages (from -r .\\requirements.txt (line 65)) (5.9.8)\n",
            "Requirement already satisfied: pure-eval==0.2.2 in c:\\users\\vitor\\appdata\\roaming\\python\\python311\\site-packages (from -r .\\requirements.txt (line 66)) (0.2.2)\n",
            "Requirement already satisfied: Pygments==2.18.0 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 67)) (2.18.0)\n",
            "Requirement already satisfied: pyparsing==3.1.2 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 68)) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil==2.9.0.post0 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 69)) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify==8.0.4 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 70)) (8.0.4)\n",
            "Requirement already satisfied: pytz==2024.1 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 71)) (2024.1)\n",
            "Requirement already satisfied: pywin32==306 in c:\\users\\vitor\\appdata\\roaming\\python\\python311\\site-packages (from -r .\\requirements.txt (line 72)) (306)\n",
            "Requirement already satisfied: PyYAML==6.0.1 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 73)) (6.0.1)\n",
            "Requirement already satisfied: regex==2024.5.15 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 74)) (2024.5.15)\n",
            "Requirement already satisfied: requests==2.32.2 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 75)) (2.32.2)\n",
            "Requirement already satisfied: rich==13.7.1 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 76)) (13.7.1)\n",
            "Requirement already satisfied: safetensors==0.4.3 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 77)) (0.4.3)\n",
            "Requirement already satisfied: scikeras==0.13.0 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 78)) (0.13.0)\n",
            "Requirement already satisfied: scikit-learn==1.5.0 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 79)) (1.5.0)\n",
            "Requirement already satisfied: scipy==1.12.0 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 80)) (1.12.0)\n",
            "Requirement already satisfied: seaborn==0.13.2 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 81)) (0.13.2)\n",
            "Requirement already satisfied: six==1.16.0 in c:\\users\\vitor\\appdata\\roaming\\python\\python311\\site-packages (from -r .\\requirements.txt (line 82)) (1.16.0)\n",
            "Requirement already satisfied: smart-open==7.0.4 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 83)) (7.0.4)\n",
            "Requirement already satisfied: stack-data==0.6.3 in c:\\users\\vitor\\appdata\\roaming\\python\\python311\\site-packages (from -r .\\requirements.txt (line 84)) (0.6.3)\n",
            "Requirement already satisfied: sympy==1.12.1 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 85)) (1.12.1)\n",
            "Requirement already satisfied: tbb==2021.12.0 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 86)) (2021.12.0)\n",
            "Requirement already satisfied: tensorboard==2.16.2 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 87)) (2.16.2)\n",
            "Requirement already satisfied: tensorboard-data-server==0.7.2 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 88)) (0.7.2)\n",
            "Requirement already satisfied: tensorflow==2.16.1 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 89)) (2.16.1)\n",
            "Requirement already satisfied: tensorflow-intel==2.16.1 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 90)) (2.16.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem==0.31.0 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 91)) (0.31.0)\n",
            "Requirement already satisfied: termcolor==2.4.0 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 92)) (2.4.0)\n",
            "Requirement already satisfied: text-unidecode==1.3 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 93)) (1.3)\n",
            "Requirement already satisfied: tf_keras==2.16.0 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 94)) (2.16.0)\n",
            "Requirement already satisfied: threadpoolctl==3.5.0 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 95)) (3.5.0)\n",
            "Requirement already satisfied: tokenizers==0.19.1 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 96)) (0.19.1)\n",
            "Requirement already satisfied: torch==2.3.0 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 97)) (2.3.0)\n",
            "Requirement already satisfied: tornado==6.4 in c:\\users\\vitor\\appdata\\roaming\\python\\python311\\site-packages (from -r .\\requirements.txt (line 98)) (6.4)\n",
            "Requirement already satisfied: tqdm==4.66.4 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 99)) (4.66.4)\n",
            "Requirement already satisfied: traitlets==5.14.3 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 100)) (5.14.3)\n",
            "Requirement already satisfied: transformers==4.41.2 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 101)) (4.41.2)\n",
            "Requirement already satisfied: typing_extensions==4.12.0 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 102)) (4.12.0)\n",
            "Requirement already satisfied: tzdata==2024.1 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 103)) (2024.1)\n",
            "Requirement already satisfied: urllib3==2.2.1 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 104)) (2.2.1)\n",
            "Requirement already satisfied: wcwidth==0.2.13 in c:\\users\\vitor\\appdata\\roaming\\python\\python311\\site-packages (from -r .\\requirements.txt (line 105)) (0.2.13)\n",
            "Requirement already satisfied: webencodings==0.5.1 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 106)) (0.5.1)\n",
            "Requirement already satisfied: Werkzeug==3.0.3 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 107)) (3.0.3)\n",
            "Requirement already satisfied: wordcloud==1.9.3 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 108)) (1.9.3)\n",
            "Requirement already satisfied: wrapt==1.16.0 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r .\\requirements.txt (line 109)) (1.16.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from astunparse==1.6.3->-r .\\requirements.txt (line 3)) (0.43.0)\n",
            "Requirement already satisfied: pyzmq>=23.0 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jupyter_client==8.6.2->-r .\\requirements.txt (line 34)) (26.0.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\vitor\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard==2.16.2->-r .\\requirements.txt (line 87)) (65.5.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install -r .\\requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0yGWmgmRyYt2"
      },
      "outputs": [],
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "import sklearn as skl\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from nltk.stem import RSLPStemmer\n",
        "import nltk\n",
        "from zipfile import ZipFile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import os\n",
        "import re\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.metrics import roc_auc_score, make_scorer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import sklearn.metrics as skl_metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZUiPQ4nDnb1",
        "outputId": "3aced4de-dbd3-4de5-8dcc-70d9c30684e9"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TgZLHFtGL0v",
        "outputId": "d56bbae6-337f-407a-ef5e-e00b7dd3454a"
      },
      "outputs": [],
      "source": [
        "# %cd /content/drive/MyDrive/af_ia/\n",
        "\n",
        "# !ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0RAebzj8yYt4"
      },
      "outputs": [],
      "source": [
        "# Caminho dos arquivos extraidos do kaggle\n",
        "path_dataset = r'files/classificao-de-notcias.zip'\n",
        "path_db = r'db'\n",
        "\n",
        "# Caminho dos arquivos que serão utilizados para a atividade\n",
        "path_train = r'db/arquivos_competicao/arquivos_competicao/train.csv'\n",
        "path_test = r'db/arquivos_competicao/arquivos_competicao/test.csv'\n",
        "path_news = r'db/arquivos_competicao/arquivos_competicao/news'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def criar_diretorios(caminho):\n",
        "    # Extrai o diretório do caminho fornecido\n",
        "    diretorio = os.path.dirname(caminho)\n",
        "    \n",
        "    # Cria os diretórios se eles não existirem\n",
        "    if not os.path.exists(diretorio):\n",
        "        os.makedirs(diretorio)\n",
        "        print(f\"{diretorio} criado com sucesso\")\n",
        "    else:\n",
        "        print(f\"{diretorio} já existe\")\n",
        "        \n",
        "    return caminho"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1-4D7HYyYt5",
        "outputId": "6a8cae84-53cc-48db-8caf-017a330f4f05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Arquivo já descompactado\n"
          ]
        }
      ],
      "source": [
        "def unzip(path, pathFolder):\n",
        "\n",
        "    # descompacta a base de dados de notícias\n",
        "    z = ZipFile(path, 'r')\n",
        "\n",
        "    if os.path.isdir(pathFolder):\n",
        "        z.extractall(pathFolder)\n",
        "        z.close()\n",
        "    else:\n",
        "        os.mkdir(pathFolder)\n",
        "        z.extractall(pathFolder)\n",
        "        z.close()\n",
        "\n",
        "    print(\"Arquivo descompactado com sucesso!\")\n",
        "\n",
        "# Antes de descompactar os arquivos valida se ja foram descompactados antes\n",
        "if not os.path.isdir(path_news):\n",
        "    unzip(path_dataset, path_db)\n",
        "else:\n",
        "    print(\"Arquivo já descompactado\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDa1FYtuyYt6"
      },
      "source": [
        "---\n",
        "## 1. Carregando os arquivos de teste e treino"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZDczMkqyYt7",
        "outputId": "01283380-0ecd-433d-973d-08165f933770"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colunas do arquivo de treino:  Index(['ID', 'Class'], dtype='object')\n",
            "Quantidade de linhas:  2781\n"
          ]
        }
      ],
      "source": [
        "#Carregando os arquivos de treino\n",
        "df_train = pd.read_csv(path_train)\n",
        "print(\"Colunas do arquivo de treino: \", df_train.columns)\n",
        "print(\"Quantidade de linhas: \", df_train.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nxifz6vcyYt8",
        "outputId": "9c1a5ada-24da-4df2-dfb3-5a7b6463604f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colunas do arquivo de treino:  Index(['ID'], dtype='object')\n",
            "Quantidade de linhas:  1193\n"
          ]
        }
      ],
      "source": [
        "# Carregando os arquivos de teste\n",
        "df_teste = pd.read_csv(path_test)\n",
        "print(\"Colunas do arquivo de treino: \", df_teste.columns)\n",
        "print(\"Quantidade de linhas: \", df_teste.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C99M5z45yYt8"
      },
      "source": [
        "---\n",
        "## Pré-Processamento dos Dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9V-Yv4mMyYt8"
      },
      "source": [
        "\n",
        "### Criando um DataFrame com os textos e titulos extraidos do XML\n",
        "Para facilitar a aplicação dos metodos foi adicionando novas colunas no DataFrame de treino e teste, contendo o texto e titulo extraidos dos arquivos xml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "oiHNPh4HyYt8"
      },
      "outputs": [],
      "source": [
        "def extract_xml_text(path_xml):\n",
        "\n",
        "    \"\"\"\n",
        "    A função `extract_xml_text` é designada para extrair os textos dos arquivos XML\n",
        "    especificados pelo `path_xml`.\n",
        "    Utilizando a biblioteca ElementTree para leitura do arquivo XML\n",
        "    \"\"\"\n",
        "\n",
        "    # Instancia um objeto como uma árvore de análise\n",
        "    tree = ET.parse(path_xml)\n",
        "\n",
        "    # Obtem o elemento raiz da árvore de ánalise\n",
        "    root = tree.getroot()\n",
        "\n",
        "    # Encontra o elemento headline (titulo) dentro da árvore de analise\n",
        "    headline = root.find('headline').text if root.find('headline') is not None else ''\n",
        "\n",
        "    # Entroa todos os elementos <p> que na estrutura dos xml's contem o texto\n",
        "    paragraphs = root.findall('.//p')\n",
        "\n",
        "    # Junta em uma unica string, separando por espaços\n",
        "    text = ' '.join([p.text for p in paragraphs if p.text is not None])\n",
        "\n",
        "    return headline, text\n",
        "\n",
        "def apply_extraction(df_applyed):\n",
        "\n",
        "    \"\"\"\n",
        "    Essa função é responsável em aplicar as novas colunas\n",
        "    no df_applyed passado como parametro.\n",
        "    \"\"\"\n",
        "\n",
        "    # Loop pelas linhas do df\n",
        "    for idx in df_applyed.index:\n",
        "\n",
        "        # atribui o valor da coluna id na variavel file\n",
        "        file = df_applyed.at[idx, 'ID']\n",
        "\n",
        "        # Concatena o nome do arquivo com o caminho dele\n",
        "        path_xml = f\"{path_news}/{file}\"\n",
        "\n",
        "        # Extrai o texto e titulo desse arquivo\n",
        "        titulo, texto = extract_xml_text(path_xml)\n",
        "\n",
        "        #Atribui esses o texto e titulos em novas colunas\n",
        "        df_applyed.at[idx, 'TITULO'] = titulo\n",
        "        df_applyed.at[idx, 'TEXTO'] = texto\n",
        "\n",
        "    return df_applyed\n",
        "\n",
        "def print_porcentagem(target):\n",
        "    # Calcula a contagem de cada classe\n",
        "    class_counts = target['Class'].value_counts()\n",
        "\n",
        "    # Calcula a porcentagem de cada classe\n",
        "    class_percentages = class_counts / len(target) * 100\n",
        "\n",
        "    # Imprime a porcentagem de cada classe\n",
        "    for cl, pct in class_percentages.items():\n",
        "        print(f\"Porcentagem da classe {cl}: {round(pct, 2)}%\")\n",
        "\n",
        "def preprocessing_portuguese(text, stemming = False, stopwords = False):\n",
        "    \"\"\"\n",
        "    Funcao usada para tratar textos escritos na lingua portuguesa\n",
        "\n",
        "    Parametros:\n",
        "        text: variavel do tipo string que contem o texto que devera ser tratado\n",
        "\n",
        "        stemming: variavel do tipo boolean que indica se a estemizacao deve ser aplicada ou nao\n",
        "\n",
        "        stopwords: variavel do tipo boolean que indica se as stopwords devem ser removidas ou nao\n",
        "    \"\"\"\n",
        "\n",
        "    # Lower case\n",
        "    text = text.lower()\n",
        "\n",
        "    # remove os acentos das palavras\n",
        "    nfkd_form = unicodedata.normalize('NFKD', text)\n",
        "    text = u\"\".join([c for c in nfkd_form if not unicodedata.combining(c)])\n",
        "\n",
        "    # remove tags HTML\n",
        "    regex = re.compile('<[^<>]+>')\n",
        "    text = re.sub(regex, \" \", text)\n",
        "\n",
        "    # normaliza as URLs\n",
        "    regex = re.compile('(http|https)://[^\\s]*')\n",
        "    text = re.sub(regex, \"<URL>\", text)\n",
        "\n",
        "    # normaliza emails\n",
        "    regex = re.compile('[^\\s]+@[^\\s]+')\n",
        "    text = re.sub(regex, \"<EMAIL>\", text)\n",
        "\n",
        "    # converte todos os caracteres não-alfanuméricos em espaço\n",
        "    regex = re.compile('[^A-Za-z0-9]+')\n",
        "    text = re.sub(regex, \" \", text)\n",
        "\n",
        "    # normaliza os numeros\n",
        "    regex = re.compile('[0-9]+.[0-9]+')\n",
        "    text = re.sub(regex, \"NUMERO\", text)\n",
        "\n",
        "    # normaliza os numeros\n",
        "    regex = re.compile('[0-9]+,[0-9]+')\n",
        "    text = re.sub(regex, \"NUMERO\", text)\n",
        "\n",
        "    # normaliza os numeros\n",
        "    regex = re.compile('[0-9]+')\n",
        "    text = re.sub(regex, \"NUMERO\", text)\n",
        "\n",
        "\n",
        "    # substitui varios espaçamentos seguidos em um só\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    # separa o texto em palavras\n",
        "    words = text.split()\n",
        "\n",
        "    # trunca o texto para apenas 200 termos\n",
        "    words = words[0:200]\n",
        "\n",
        "    # remove stopwords\n",
        "    if stopwords:\n",
        "        words = text.split() # separa o texto em palavras\n",
        "        words = [w for w in words if not w in nltk.corpus.stopwords.words('portuguese')]\n",
        "        text = \" \".join( words )\n",
        "\n",
        "    # aplica estemização\n",
        "    if stemming:\n",
        "        stemmer_method = RSLPStemmer()\n",
        "        words = [ stemmer_method.stem(w) for w in words ]\n",
        "        text = \" \".join( words )\n",
        "\n",
        "    # remove palavras compostas por apenas um caracter\n",
        "    words = text.split() # separa o texto em palavras\n",
        "    words = [ w for w in words if len(w)>1 ]\n",
        "    text = \" \".join( words )\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_FiAzU-yYt9",
        "outputId": "be7b448f-0c73-47bf-d72b-491a1497e808"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Porcentagem da classe Mercados: 74.33%\n",
            "Porcentagem da classe Economia: 21.18%\n",
            "Porcentagem da classe GovSocial: 3.24%\n",
            "Porcentagem da classe CorpIndustrial: 1.26%\n"
          ]
        }
      ],
      "source": [
        "df_train = apply_extraction(df_train)\n",
        "df_train = df_train[['ID', 'TITULO', 'TEXTO', 'Class']]\n",
        "print_porcentagem(df_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "jfbgR4TxNkqo"
      },
      "outputs": [],
      "source": [
        "# Transformação do target\n",
        "le = LabelEncoder()\n",
        "df_train['Class'] = le.fit_transform(df_train['Class'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "7hycTR3pyYt-"
      },
      "outputs": [],
      "source": [
        "df_teste = apply_extraction(df_teste)\n",
        "df_teste = df_teste[['ID', 'TITULO', 'TEXTO']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MM31fmfyYt-"
      },
      "source": [
        "### Tratando os textos da base de dados\n",
        "- Aplicada a função de estemização para a linguagem dos textos (português)\n",
        "- Removendo os ascentos das palavras\n",
        "- Criando um limite de 200 temrmos por palavras, para evitar que a predição do classificador seja influenciada pelo tamanho da noticia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bd8939tcyYt_",
        "outputId": "debabfb6-8697-4ec4-d23c-9affff13d40c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\vitor\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package rslp to\n",
            "[nltk_data]     C:\\Users\\vitor\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package rslp is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Download the stopwords corpus\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Download the RSLPStemmer\n",
        "nltk.download('rslp')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "eJJxbTBlyYt_"
      },
      "outputs": [],
      "source": [
        "# Aplicar a função ao DataFrame de treino\n",
        "df_train['TEXTO'] = df_train['TEXTO'].apply(preprocessing_portuguese)\n",
        "\n",
        "# Aplicando a função no df de teste\n",
        "df_teste['TEXTO'] = df_teste['TEXTO'].apply(preprocessing_portuguese)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EumSz2CAyYt_"
      },
      "source": [
        "### Fazendo a divisão entre os dados de teste e treino"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6emovbfvyYuA",
        "outputId": "281543bb-fc16-4e91-91a2-76bef9f62414"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Qtd. dados de treinamento: 2224 (79.97%)\n",
            "Qtd. de dados de teste: 557 (20.03%)\n"
          ]
        }
      ],
      "source": [
        "# gera uma divisão dos dados em treino e teste\n",
        "cv = skl.model_selection.StratifiedShuffleSplit(n_splits=1, train_size=0.8,\n",
        "                                                random_state=10)\n",
        "\n",
        "# retorna os índices de treino e teste\n",
        "dataset = df_train['TEXTO']\n",
        "target = df_train['Class']\n",
        "train_index, test_index = list( cv.split(dataset, target) )[0]\n",
        "\n",
        "# retorna as partições de treino e teste de acordo com os índices\n",
        "dataset_train, dataset_test = dataset[train_index], dataset[test_index]\n",
        "Y_train, Y_test = target[train_index], target[test_index]\n",
        "\n",
        "print('Qtd. dados de treinamento: %d (%1.2f%%)' %(dataset_train.shape[0], (dataset_train.shape[0]/dataset.shape[0])*100) )\n",
        "print('Qtd. de dados de teste: %d (%1.2f%%)' %(dataset_test.shape[0], (dataset_test.shape[0]/dataset.shape[0])*100) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVG402s3yYuA"
      },
      "source": [
        "### Gerando a representação vetorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fuu_UtNZDsPz"
      },
      "source": [
        "#### Gerando a representação vetorial para TF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIqy8VyRyYuA"
      },
      "source": [
        "Iremos transformar o texto em um vetor de atributos com valores numéricos. Uma das formas de fazer isso é considerar que cada palavra (ou token) da base de dados de treinamento é um atributo que armazena o número de vezes que uma determinada palavra aparece no texto. Na biblioteca `scikit-learn` podemos fazer essa conversão de texto para um vetor de atributos usando a função `skl.feature_extraction.text.CountVectorizer()`. Essa função gera um modelo de vetorização que pode ser ajustado com a base nos dados de treinamento usando a função `fit_transform()`.\n",
        "\n",
        "Obs.: você deve treinar o modelo de representação **apenas com os dados de treinamento**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7As4k03yYuA",
        "outputId": "4f43e59c-3c7b-4797-b044-d9537f560400"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20 primeiras palavras do vocabulário obtidas a partir dos dados de treinamento:\n",
            "\n",
            "['aa' 'aaa' 'aanumero' 'aas' 'abaixo' 'abaixos' 'abaixou' 'abalada'\n",
            " 'abanadas' 'abanar' 'abandona' 'abandonado' 'abandonaram' 'abastecimento'\n",
            " 'abatidas' 'abatimentos' 'aberta' 'abertamente' 'abertas' 'aberto']\n",
            "\n",
            "Dimensão dos dados vetorizados:  (2224, 9506)\n",
            "\n",
            "Dimensão dos dados vetorizados:  (557, 9506)\n"
          ]
        }
      ],
      "source": [
        "# inicializa o modelo usado para gerar a representação TF (term frequency)\n",
        "vectorizer = skl.feature_extraction.text.CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None,\n",
        "                                                         stop_words = None, lowercase = True, binary=False, dtype=np.int32)\n",
        "\n",
        "# treina o modelo TF com os dados de treinamento e converte os dados de treinamento para uma array que contém a frequência dos termos em cada documento (TF - term frequency)\n",
        "X_train_tf = vectorizer.fit_transform(dataset_train)\n",
        "\n",
        "# converte os dados de teste\n",
        "X_test_tf = vectorizer.transform(dataset_test)\n",
        "\n",
        "print('20 primeiras palavras do vocabulário obtidas a partir dos dados de treinamento:\\n')\n",
        "print(vectorizer.get_feature_names_out()[0:20])\n",
        "\n",
        "print('\\nDimensão dos dados vetorizados: ', X_train_tf.shape)\n",
        "print('\\nDimensão dos dados vetorizados: ', X_test_tf.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vmi529yiELcH"
      },
      "source": [
        "#### Gerando a representação vetorial para binário"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KP7zAiCtEQco"
      },
      "source": [
        "\n",
        "Utilizando a técnica de vetorização binária onde cada termo é representado como um valor binário. Ou seja, se o termo aparece no documento o valor é 1, caso contrário o valor é 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUx6Sm9bEPpj",
        "outputId": "925834d8-3053-414c-ae6e-03d29722e14b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2224, 9506)\n",
            "(557, 9506)\n"
          ]
        }
      ],
      "source": [
        "X_train_bin = X_train_tf.copy()\n",
        "X_test_bin = X_test_tf.copy()\n",
        "\n",
        "#convert os dados de treino para representação binária\n",
        "X_train_bin[X_train_bin!=0]=1\n",
        "\n",
        "#convert os dados de teste para representação binária\n",
        "X_test_bin[X_test_bin!=0]=1\n",
        "\n",
        "print(X_train_bin.shape)\n",
        "print(X_test_bin.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHAatdSQEUjn"
      },
      "source": [
        "#### TF-IDF\n",
        "\n",
        "Utilizando a função **TfidfTransformer** para para converter a representação vetorial TF para TF-IDF\n",
        "Os parametros utilizados nessa função foram:\n",
        "- **norm=l2**: normalizando cada vetor TF-IDF para que a soma dos quadrados dos elementos seja igual a 1\n",
        "- **smooth_idf**: adiciona 1 ao denominador da formula de IDF para eviter divisoes por zero\n",
        "- **sublinear_tf**: aplica a sublinearização do TF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AaoLI4dEXzn",
        "outputId": "255e2691-9145-43e9-aa60-0ed508b7e3cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2224, 9506)\n",
            "(557, 9506)\n"
          ]
        }
      ],
      "source": [
        "#Inicializa o modelo usado para gerar a representação TF-IDF\n",
        "tfidf_model = skl.feature_extraction.text.TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
        "\n",
        "# Treina o modelo com os vetores de treinamento\n",
        "X_train_tfidf = tfidf_model.fit_transform(X_train_tf)\n",
        "\n",
        "# treina o modelo com os dados de teste\n",
        "X_test_tfidf = tfidf_model.transform(X_test_tf)\n",
        "\n",
        "print(X_train_tfidf.shape)\n",
        "print(X_test_tfidf.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrZ8PMHrEdOd"
      },
      "source": [
        "### Gerando word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8h1ysItyYuB"
      },
      "source": [
        "\n",
        "#### Gerando com a própria base\n",
        "\n",
        "Depois de fazer o pré-processamento, é necessário transformar o texto em um vetor de atributos com valores numéricos. Podemos fazer isso usando word embeddings pré-treinadas ou treinando um modelo próprio.\n",
        "\n",
        "Vamos passar por toda a base de dados e transformar cada documento em uma lista de palavra."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEBJocaByYuB",
        "outputId": "c68f7ecf-a50a-4bdd-be8d-09e378d81de1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "20 primeiras palavras da primeira amostra de treino\n",
            "['lisboa', 'NUMERO', 'mai', 'reuter', 'indice', 'de', 'precos', 'dos', 'bens', 'nao', 'transaccionaveis', 'foi', 'em', 'abril', 'de', 'NUMERO', 'pct', 'contra', 'NUMERO', 'pct', 'no', 'mes', 'de', 'marco', 'anunciou', 'hoje', 'instituto', 'nacional', 'de', 'estatistica']\n",
            "\n",
            "\n",
            "20 primeiras palavras da primeira amostra de teste\n",
            "['lisboa', 'NUMERO', 'jun', 'reuter', 'indice', 'de', 'precos', 'no', 'consumidor', 'ipc', 'devera', 'registar', 'uma', 'evolucao', 'moderada', 'apesar', 'de', 'em', 'maio', 'ipc', 'homologo', 'ter', 'invertido', 'tendencia', 'descendente', 'dos', 'meses', 'anteriores', 'refere', 'sintese']\n"
          ]
        }
      ],
      "source": [
        "dataset2_train = []\n",
        "for i, msg in enumerate(dataset_train):\n",
        "    dataset2_train.append(msg.split())\n",
        "\n",
        "dataset2_test = []\n",
        "for i, msg in enumerate(dataset_test):\n",
        "    dataset2_test.append(msg.split())\n",
        "\n",
        "print(\"\\n\\n20 primeiras palavras da primeira amostra de treino\")\n",
        "print(dataset2_train[0][0:30])\n",
        "\n",
        "print(\"\\n\\n20 primeiras palavras da primeira amostra de teste\")\n",
        "print(dataset2_test[0][0:30])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykcNNMfByYuB"
      },
      "source": [
        "Neste momento, vamos treinar modelo próprio de embeddings baseado nos dados de treinamento. Para manipular as embeddings, iremos usar a biblioteca Gensim: https://radimrehurek.com/gensim/models/word2vec.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40w8HdNtyYuB",
        "outputId": "19218172-8746-4fa5-d7c8-715552c55657"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Tamanho do vocabulário do modelo:  9507\n"
          ]
        }
      ],
      "source": [
        "sentencasEmbedding = dataset2_train\n",
        "embeddingModel = Word2Vec(sentences = sentencasEmbedding,\n",
        "                          vector_size = 200,\n",
        "                          window = 3,\n",
        "                          min_count = 1)\n",
        "\n",
        "vocabSize = len(embeddingModel.wv)\n",
        "\n",
        "print(\"\\nTamanho do vocabulário do modelo: \", vocabSize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "PZUwlgGgyYuC"
      },
      "outputs": [],
      "source": [
        "def getDocvector(model, doc):\n",
        "    \"\"\"\n",
        "    obtem o vetor de cada palavra de um documento e calcula um vetor medio\n",
        "    \"\"\"\n",
        "\n",
        "    wordList = []\n",
        "    for word in doc:\n",
        "\n",
        "        try:\n",
        "            vec = model.wv[word]\n",
        "            wordList.append(vec)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    if len(wordList)>0:\n",
        "        vetorMedio = np.mean( wordList, axis=0 )\n",
        "    else:\n",
        "        vetorMedio = np.zeros( model.wv.vector_size )\n",
        "\n",
        "    return vetorMedio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOdz4sZGyYuC",
        "outputId": "46b78a55-1de7-491b-f45a-9e4bd1d3f3e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2224, 200)\n",
            "(557, 200)\n"
          ]
        }
      ],
      "source": [
        "def dataset2featureMatrix(dataset, embeddingModel):\n",
        "\n",
        "    X_embedding = []\n",
        "    for doc in dataset:\n",
        "        vec = getDocvector(embeddingModel, doc)\n",
        "        X_embedding.append(vec)\n",
        "\n",
        "    X_embedding = np.array(X_embedding)\n",
        "    return X_embedding\n",
        "\n",
        "\n",
        "X_train_embedding = dataset2featureMatrix(dataset2_train, embeddingModel)\n",
        "X_test_embedding = dataset2featureMatrix(dataset2_test, embeddingModel)\n",
        "\n",
        "print(X_train_embedding.shape)\n",
        "print(X_test_embedding.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6MKEqRiyYuC"
      },
      "source": [
        "_______\n",
        "# Treinando um modelo de classificação por Naive Bayes Multinomial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "OmAgnxBJyYuD"
      },
      "outputs": [],
      "source": [
        "def classifica(X_train, X_test,\n",
        "                Y_train, Y_test,\n",
        "                formato = \"TF\"):\n",
        "\n",
        "  model = MultinomialNB(\n",
        "      alpha=0.8, fit_prior=True, force_alpha=True\n",
        "  )\n",
        "\n",
        "  # normaliza os dados\n",
        "  if formato==\"embedding\":\n",
        "    scaler = skl.preprocessing.MinMaxScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "  elif formato==\"TF\":\n",
        "    scaler = skl.preprocessing.Normalizer(norm='l2')\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "  # treinando o método de classificação\n",
        "  model.fit( X_train, Y_train )\n",
        "\n",
        "  # classifica os dados de teste\n",
        "  Y_pred = model.predict(X_test)\n",
        "\n",
        "  print(\"\\nAcurácia: \", skl.metrics.accuracy_score(Y_test, Y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoZ9uwmxOxaV",
        "outputId": "b0f5ec28-965e-4ea4-889c-848e1534d03c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classificação Binário\n",
            "\n",
            "Acurácia:  0.9694793536804309\n",
            "Classificação TF\n",
            "\n",
            "Acurácia:  0.7432675044883303\n",
            "\n",
            "Classificação TF-IDF\n",
            "\n",
            "Acurácia:  0.8779174147217235\n",
            "\n",
            "Classificação word embbeddings\n",
            "\n",
            "Acurácia:  0.7989228007181328\n"
          ]
        }
      ],
      "source": [
        "print(\"Classificação Binário\")\n",
        "classifica(X_train_bin, X_test_bin, Y_train, Y_test,\n",
        "            formato = \"binario\")\n",
        "\n",
        "print(\"Classificação TF\")\n",
        "classifica(X_train_tf, X_test_tf, Y_train, Y_test,\n",
        "            formato = \"TF\")\n",
        "\n",
        "print(\"\\nClassificação TF-IDF\")\n",
        "classifica(X_train_tfidf, X_test_tfidf, Y_train, Y_test,\n",
        "            formato = \"TF\")\n",
        "\n",
        "print(\"\\nClassificação word embbeddings\")\n",
        "classifica(X_train_embedding, X_test_embedding, Y_train, Y_test,\n",
        "            formato = \"embedding\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebc0hwaEP0ZI"
      },
      "source": [
        "# Escolha dos Hiperparâmetros\n",
        "\n",
        "grid search para definir o alpha"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "X0QaUG4dP0Ld"
      },
      "outputs": [],
      "source": [
        "def buscar_melhores_hiperparametros(X_train, Y_train):\n",
        "    # Definindo o modelo\n",
        "    model = MultinomialNB(fit_prior=True, force_alpha=True)\n",
        "\n",
        "    # Definindo os valores dos hiperparâmetros para testar\n",
        "    param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
        "\n",
        "    # Definindo o scorer AUC para multiclasse\n",
        "    auc_scorer = make_scorer(roc_auc_score, needs_proba=True, multi_class='ovr')\n",
        "\n",
        "    # Inicializando a busca em grade com validação cruzada\n",
        "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring=auc_scorer)\n",
        "\n",
        "    # Treinando o modelo com a busca em grade\n",
        "    grid_search.fit(X_train, Y_train)\n",
        "\n",
        "    # Retorna o melhor modelo e o melhor valor de 'C' encontrado\n",
        "    return grid_search.best_estimator_, grid_search.best_params_['alpha']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "F-iccDdJQOcv"
      },
      "outputs": [],
      "source": [
        "def classificar_com_melhores_hiperparametros(X_train, X_test, Y_train, Y_test, formato):\n",
        "    # Normaliza os dados\n",
        "    if formato==\"embedding\":\n",
        "      scaler = skl.preprocessing.MinMaxScaler()\n",
        "      X_train = scaler.fit_transform(X_train)\n",
        "      X_test = scaler.transform(X_test)\n",
        "\n",
        "    elif formato==\"TF\":\n",
        "      scaler = skl.preprocessing.Normalizer(norm='l2')\n",
        "      X_train = scaler.fit_transform(X_train)\n",
        "      X_test = scaler.transform(X_test)\n",
        "\n",
        "    # Encontrando os melhores hiperparâmetros\n",
        "    best_model, melhores_parametros = buscar_melhores_hiperparametros(X_train, Y_train)\n",
        "    print(f\"Melhores Hiperparâmetros: {melhores_parametros}\")\n",
        "\n",
        "    # Treinando o método de classificação com os melhores hiperparâmetros\n",
        "    best_model.fit(X_train, Y_train)\n",
        "\n",
        "    # Classifica os dados de teste\n",
        "    Y_pred = best_model.predict(X_test)\n",
        "\n",
        "    # Relatório de classificação\n",
        "    print(\"\\nRelatório de Classificação:\\n\", classification_report(Y_test, Y_pred))\n",
        "\n",
        "    # Imprime a acurácia e AUC\n",
        "    accuracy = skl_metrics.accuracy_score(Y_test, Y_pred)\n",
        "    print(f\"Acurácia: {accuracy:.4f}\")\n",
        "\n",
        "    y_proba = best_model.predict_proba(X_test)\n",
        "    auc = roc_auc_score(Y_test, y_proba, multi_class='ovo')\n",
        "    print(f\"AUC: {auc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixZegBzeQfvn"
      },
      "source": [
        "#### Testando TF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TQEiVWmQnOo",
        "outputId": "8df3743e-6135-42df-f360-7f98f0080745"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\vitor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py:610: FutureWarning: The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Melhores Hiperparâmetros: 0.001\n",
            "\n",
            "Relatório de Classificação:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.29      0.44         7\n",
            "           1       0.89      0.87      0.88       118\n",
            "           2       0.91      0.56      0.69        18\n",
            "           3       0.96      1.00      0.98       414\n",
            "\n",
            "    accuracy                           0.95       557\n",
            "   macro avg       0.94      0.68      0.75       557\n",
            "weighted avg       0.95      0.95      0.94       557\n",
            "\n",
            "Acurácia: 0.9461\n",
            "AUC: 0.9314\n"
          ]
        }
      ],
      "source": [
        "best_model_tf_grid = classificar_com_melhores_hiperparametros(X_train_tf, X_test_tf, Y_train, Y_test, 'TF')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UeLQ-19QgQu"
      },
      "source": [
        "#### Testando TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqR-nhbkQpS5",
        "outputId": "ac61b85d-584d-4fca-ba2c-4d863dec2ca4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\vitor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py:610: FutureWarning: The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Melhores Hiperparâmetros: 0.01\n",
            "\n",
            "Relatório de Classificação:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.43      0.55         7\n",
            "           1       0.93      0.95      0.94       118\n",
            "           2       0.82      0.78      0.80        18\n",
            "           3       0.99      0.99      0.99       414\n",
            "\n",
            "    accuracy                           0.97       557\n",
            "   macro avg       0.87      0.79      0.82       557\n",
            "weighted avg       0.97      0.97      0.97       557\n",
            "\n",
            "Acurácia: 0.9695\n",
            "AUC: 0.9494\n"
          ]
        }
      ],
      "source": [
        "best_model_tfidf_grid = classificar_com_melhores_hiperparametros(X_train_tfidf, X_test_tfidf, Y_train, Y_test, 'TF')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aG7bG67YQhap"
      },
      "source": [
        "#### Testando Binário"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VM2vwzhDQqdw",
        "outputId": "cd42a25f-0954-4363-b5b5-1d535bd656a8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\vitor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py:610: FutureWarning: The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Melhores Hiperparâmetros: 0.1\n",
            "\n",
            "Relatório de Classificação:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.43      0.50         7\n",
            "           1       0.97      0.94      0.95       118\n",
            "           2       0.73      0.89      0.80        18\n",
            "           3       0.99      0.99      0.99       414\n",
            "\n",
            "    accuracy                           0.97       557\n",
            "   macro avg       0.82      0.81      0.81       557\n",
            "weighted avg       0.97      0.97      0.97       557\n",
            "\n",
            "Acurácia: 0.9713\n",
            "AUC: 0.9578\n"
          ]
        }
      ],
      "source": [
        "best_model_tfbin_grid = classificar_com_melhores_hiperparametros(X_train_bin, X_test_bin, Y_train, Y_test, 'binario')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wp_dUnkQit6"
      },
      "source": [
        "#### Testando Word embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCR4zgcmQsla",
        "outputId": "980abbe6-ffec-40ec-ff54-10a2d335db32"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\vitor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py:610: FutureWarning: The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Melhores Hiperparâmetros: 1\n",
            "\n",
            "Relatório de Classificação:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         7\n",
            "           1       0.56      0.64      0.60       118\n",
            "           2       0.30      0.89      0.45        18\n",
            "           3       0.95      0.86      0.90       414\n",
            "\n",
            "    accuracy                           0.80       557\n",
            "   macro avg       0.45      0.59      0.49       557\n",
            "weighted avg       0.84      0.80      0.81       557\n",
            "\n",
            "Acurácia: 0.7989\n",
            "AUC: 0.8528\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\vitor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\vitor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\vitor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "best_model_embedding_grid = classificar_com_melhores_hiperparametros(X_train_embedding, X_test_embedding, Y_train, Y_test, 'embedding')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeXU1MtxdSTK"
      },
      "source": [
        "---\n",
        "# Previsão do conjunto de teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "QAZq-wFPdTAB"
      },
      "outputs": [],
      "source": [
        "# Função para preparar os dados de teste\n",
        "def prepare_test_data(df_test):\n",
        "    # Aplicar a função de extração de textos do XML no DataFrame de teste\n",
        "    df_test = apply_extraction(df_test)\n",
        "\n",
        "    # Re-organizar as colunas do DataFrame\n",
        "    df_test = df_test[['ID', 'TITULO', 'TEXTO']]\n",
        "\n",
        "    # Aplicar a função de pré-processamento no texto das amostras de teste\n",
        "    df_test['TEXTO'] = df_test['TEXTO'].apply(preprocessing_portuguese)\n",
        "\n",
        "    return df_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Tda79tCKdUh6"
      },
      "outputs": [],
      "source": [
        "def classificar_e_treinar_com_melhores_hiperparametros(X_train, X_val, y_train, y_val):\n",
        "    # Encontrando os melhores hiperparâmetros\n",
        "    best_model, melhores_parametros = buscar_melhores_hiperparametros(X_train, y_train)\n",
        "    print(f\"Melhores Hiperparâmetros: {melhores_parametros}\")\n",
        "\n",
        "    # Fazendo previsões no conjunto de validação\n",
        "    y_pred = best_model.predict(X_val)\n",
        "\n",
        "    # Avaliando a acurácia\n",
        "    accuracy = skl_metrics.accuracy_score(y_val, y_pred)\n",
        "    print(f\"Acurácia: {accuracy:.4f}\")\n",
        "\n",
        "    # Relatório de classificação\n",
        "    print(\"\\nRelatório de Classificação:\\n\", skl_metrics.classification_report(y_val, y_pred,zero_division=0))\n",
        "\n",
        "    # Matriz de confusão\n",
        "    print(\"\\nMatriz de Confusão:\\n\", skl_metrics.confusion_matrix(y_val, y_pred))\n",
        "\n",
        "    # Calculando a AUC\n",
        "    y_proba = best_model.predict_proba(X_val)\n",
        "    auc = roc_auc_score(y_val, y_proba, multi_class='ovo')\n",
        "    print(f\"AUC: {auc:.4f}\")\n",
        "\n",
        "    return best_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "elqn9vMhdkf1"
      },
      "outputs": [],
      "source": [
        "# Função para obter as probabilidades e gerar o arquivo de submissão\n",
        "def generate_submission_file(classifier, X_test, df_test, filename='submission.csv'):\n",
        "    # Obter as probabilidades das classes\n",
        "    y_proba = classifier.predict_proba(X_test)\n",
        "\n",
        "    # Criar um DataFrame com as probabilidades\n",
        "    submission_df = pd.DataFrame(y_proba, columns=le.classes_)\n",
        "\n",
        "    # Adicionar a coluna ID\n",
        "    submission_df.insert(0, 'ID', df_test['ID'])\n",
        "\n",
        "    # Renomear as colunas para o formato exigido\n",
        "    submission_df.columns = ['ID', 'CorpIndustrial', 'Economia', 'GovSocial', 'Mercados']\n",
        "\n",
        "    # Salvar o DataFrame como um arquivo CSV\n",
        "    submission_df.to_csv(filename, index=False, float_format='%.5f')\n",
        "    print(f\"Arquivo de submissão salvo como {filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "As-bfB1zdoNZ"
      },
      "outputs": [],
      "source": [
        "# Carregar e preparar os dados de teste\n",
        "df_test = prepare_test_data(df_teste)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "i_ejJM7gdpni"
      },
      "outputs": [],
      "source": [
        "# Como o Binario teve a maior AUC, então será aplicado apenas esse metodo\n",
        "X_test_tf = vectorizer.transform(df_test['TEXTO'])\n",
        "X_test_bin = X_test_tf.copy()\n",
        "\n",
        "#convert os dados de teste para representação binária\n",
        "X_test_bin[X_test_bin!=0]=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDVgLHkieA-C",
        "outputId": "5f73966a-5e49-4c23-e8e8-44cb63596c59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Treinando com o formato Binario usando busca por grid\n",
            "(1779, 9506) (445, 9506) (1779,)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\vitor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py:610: FutureWarning: The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Melhores Hiperparâmetros: 0.1\n",
            "Acurácia: 0.9461\n",
            "\n",
            "Relatório de Classificação:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.44      0.67      0.53         6\n",
            "           1       0.98      0.85      0.91        94\n",
            "           2       0.48      1.00      0.65        14\n",
            "           3       0.99      0.98      0.98       331\n",
            "\n",
            "    accuracy                           0.95       445\n",
            "   macro avg       0.72      0.87      0.77       445\n",
            "weighted avg       0.97      0.95      0.95       445\n",
            "\n",
            "\n",
            "Matriz de Confusão:\n",
            " [[  4   0   2   0]\n",
            " [  1  80  11   2]\n",
            " [  0   0  14   0]\n",
            " [  4   2   2 323]]\n",
            "AUC: 0.9789\n"
          ]
        }
      ],
      "source": [
        "# Divisão dos dados de treino em treino e validação\n",
        "X_train_part, X_val_part, y_train_part, y_val_part = train_test_split(X_train_bin, Y_train, test_size=0.2, random_state=42, stratify=Y_train)\n",
        "\n",
        "# Treinar e avaliar o modelo com busca aleatória para Binario\n",
        "print(\"\\n\\nTreinando com o formato Binario usando busca por grid\")\n",
        "print(X_train_part.shape, X_val_part.shape, y_train_part.shape)\n",
        "best_model_bin = classificar_e_treinar_com_melhores_hiperparametros(X_train_part, X_val_part, y_train_part, y_val_part)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17qYsbGRemVd",
        "outputId": "b5e66b3f-082c-4244-f725-c91ac923a3b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "submission_files/naive já existe\n",
            "Arquivo de submissão salvo como submission_files/naive/submission_bin.csv\n"
          ]
        }
      ],
      "source": [
        "# Gerar o arquivo de submissão usando o modelo treinado\n",
        "path_submission = criar_diretorios('submission_files/naive/')\n",
        "generate_submission_file(best_model_bin, X_test_bin, df_test, filename=path_submission + 'submission_bin.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "wrZ8PMHrEdOd"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
