{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from sklearn import feature_extraction\n",
    "import sklearn as skl\n",
    "from nltk.stem import RSLPStemmer\n",
    "import nltk\n",
    "from zipfile import ZipFile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import os\n",
    "import re\n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminho dos arquivos extraidos do kaggle\n",
    "path_dataset = r'files\\classificao-de-notcias.zip'\n",
    "path_db = r'db'\n",
    "\n",
    "# Caminho dos arquivos que serão utilizados para a atividade\n",
    "path_train = r'db\\arquivos_competicao\\arquivos_competicao\\train.csv'\n",
    "path_test = r'db\\arquivos_competicao\\arquivos_competicao\\test.csv'\n",
    "path_news = r'db\\arquivos_competicao\\arquivos_competicao\\news'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vitor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package rslp to\n",
      "[nltk_data]     C:\\Users\\vitor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the stopwords corpus\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Download the RSLPStemmer\n",
    "nltk.download('rslp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraindo os dados do arquivo .zip, baixado do kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo já descompactado\n"
     ]
    }
   ],
   "source": [
    "def unzip(path, pathFolder):\n",
    "\n",
    "    # descompacta a base de dados de notícias\n",
    "    z = ZipFile(path, 'r')\n",
    "\n",
    "    if os.path.isdir(pathFolder):\n",
    "        z.extractall(pathFolder)\n",
    "        z.close()\n",
    "    else:\n",
    "        os.mkdir(pathFolder)\n",
    "        z.extractall(pathFolder)\n",
    "        z.close()\n",
    "\n",
    "    print(\"Arquivo descompactado com sucesso!\")\n",
    "    \n",
    "# Antes de descompactar os arquivos valida se ja foram descompactados antes\n",
    "if not os.path.isdir(path_news):\n",
    "    unzip(path_dataset, path_db)\n",
    "else:\n",
    "    print(\"Arquivo já descompactado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando os arquivos de teste e treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>news_00002.xml</td>\n",
       "      <td>Mercados</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>news_00003.xml</td>\n",
       "      <td>Mercados</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>news_00006.xml</td>\n",
       "      <td>Mercados</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>news_00007.xml</td>\n",
       "      <td>Economia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>news_00008.xml</td>\n",
       "      <td>Mercados</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID     Class\n",
       "0  news_00002.xml  Mercados\n",
       "1  news_00003.xml  Mercados\n",
       "2  news_00006.xml  Mercados\n",
       "3  news_00007.xml  Economia\n",
       "4  news_00008.xml  Mercados"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Carregando os arquivos de treino\n",
    "df_train = pd.read_csv(path_train)\n",
    "df_train = df_train.sort_values(['ID'])\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>news_00001.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>news_00004.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>news_00005.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>news_00011.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>news_00015.xml</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID\n",
       "0  news_00001.xml\n",
       "1  news_00004.xml\n",
       "2  news_00005.xml\n",
       "3  news_00011.xml\n",
       "4  news_00015.xml"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carregando os arquivos de teste\n",
    "df_teste = pd.read_csv(path_test)\n",
    "df_teste = df_teste.sort_values(['ID'])\n",
    "df_teste.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando um DataFrame com os textos e titulos extraidos do XML\n",
    "- Para facilitar a aplicação dos metodos foi adicionando novas colunas no df de treino e teste, contem o texto e titulo extraidos dos arquivos xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_xml_text(path_xml):\n",
    "    tree = ET.parse(path_xml)\n",
    "    root = tree.getroot()\n",
    "    headline = root.find('headline').text if root.find('headline') is not None else ''\n",
    "    paragraphs = root.findall('.//p')\n",
    "    text = ' '.join([p.text for p in paragraphs if p.text is not None])\n",
    "\n",
    "    return headline, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TITULO</th>\n",
       "      <th>TEXTO</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>news_00002.xml</td>\n",
       "      <td>FUTURO OT DEZ/96 CAI PARA 102,23 CONTRA 102,5...</td>\n",
       "      <td>*Futuro (BDPOTZ6)Dez/96 102,23 vs 102,52 no f...</td>\n",
       "      <td>Mercados</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>news_00003.xml</td>\n",
       "      <td>RESUMO TRANSACÇÕES NO MERCADO CONTÍNUO.</td>\n",
       "      <td>LISBOA, 30 Set (Reuter) - Transacções no ...</td>\n",
       "      <td>Mercados</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>news_00006.xml</td>\n",
       "      <td>INDICE PSI20 SOBE 1,28 PONTOS PARA 4.764,93.</td>\n",
       "      <td>LISBOA, 24 Out (Reuter) - O índice PSI20 subi...</td>\n",
       "      <td>Mercados</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>news_00007.xml</td>\n",
       "      <td>ÍNDICE PREÇOS PRODUÇÃO INDUSTRIAL AGO 1996 SO...</td>\n",
       "      <td>LISBOA, 29 Out (Reuter) - O Índice de Pre...</td>\n",
       "      <td>Economia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>news_00008.xml</td>\n",
       "      <td>BDP INTERROMPIDA POR PROBLEMAS COMUNICAÇÕES.</td>\n",
       "      <td>LISBOA, 30 Set (Reuter) - A negociação na Bol...</td>\n",
       "      <td>Mercados</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID                                             TITULO  \\\n",
       "0  news_00002.xml   FUTURO OT DEZ/96 CAI PARA 102,23 CONTRA 102,5...   \n",
       "1  news_00003.xml           RESUMO TRANSACÇÕES NO MERCADO CONTÍNUO.    \n",
       "2  news_00006.xml      INDICE PSI20 SOBE 1,28 PONTOS PARA 4.764,93.    \n",
       "3  news_00007.xml   ÍNDICE PREÇOS PRODUÇÃO INDUSTRIAL AGO 1996 SO...   \n",
       "4  news_00008.xml      BDP INTERROMPIDA POR PROBLEMAS COMUNICAÇÕES.    \n",
       "\n",
       "                                               TEXTO     Class  \n",
       "0   *Futuro (BDPOTZ6)Dez/96 102,23 vs 102,52 no f...  Mercados  \n",
       "1       LISBOA, 30 Set (Reuter) - Transacções no ...  Mercados  \n",
       "2   LISBOA, 24 Out (Reuter) - O índice PSI20 subi...  Mercados  \n",
       "3       LISBOA, 29 Out (Reuter) - O Índice de Pre...  Economia  \n",
       "4   LISBOA, 30 Set (Reuter) - A negociação na Bol...  Mercados  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aplicando a função para o df de treino\n",
    "for idx in df_train.index:\n",
    "    file = df_train.at[idx, 'ID']\n",
    "    path_xml = f\"{path_news}\\{file}\"\n",
    "    titulo, texto = extract_xml_text(path_xml)\n",
    "    df_train.at[idx, 'TITULO'] = titulo\n",
    "    df_train.at[idx, 'TEXTO'] = texto\n",
    "\n",
    "df_train = df_train[['ID', 'TITULO', 'TEXTO', 'Class']]\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TITULO</th>\n",
       "      <th>TEXTO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>news_00001.xml</td>\n",
       "      <td>PROVÁVEL INFLAÇÃO RETOME TENDENCIA DESCENDENT...</td>\n",
       "      <td>LISBOA 12 Set (Reuter) - A inflação deverá re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>news_00004.xml</td>\n",
       "      <td>MMI TRANSACCIONA 178,129 MC, FUTUROS FAZEM 6 ...</td>\n",
       "      <td>LISBOA, 30 Set (Reuter) - O Mercado Monetário...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>news_00005.xml</td>\n",
       "      <td>ACÇÕES SEGUEM POUCO VOLÁTEIS, APATIA DEVE MAN...</td>\n",
       "      <td>LISBOA, 17 Out (Reuter) - As acções do Contín...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>news_00011.xml</td>\n",
       "      <td>ESCUDO SEGUE ESTÁVEL E APÁTICO NA MEIA SESSÃO.</td>\n",
       "      <td>O escudo seguia relativamente estável na meia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>news_00015.xml</td>\n",
       "      <td>MMI TRANSACCIONA 234,749 MC, TMP O/N 7,2707 P...</td>\n",
       "      <td>LISBOA, 20 Ago (Reuter) - O Mercado Monetário...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID                                             TITULO  \\\n",
       "0  news_00001.xml   PROVÁVEL INFLAÇÃO RETOME TENDENCIA DESCENDENT...   \n",
       "1  news_00004.xml   MMI TRANSACCIONA 178,129 MC, FUTUROS FAZEM 6 ...   \n",
       "2  news_00005.xml   ACÇÕES SEGUEM POUCO VOLÁTEIS, APATIA DEVE MAN...   \n",
       "3  news_00011.xml    ESCUDO SEGUE ESTÁVEL E APÁTICO NA MEIA SESSÃO.    \n",
       "4  news_00015.xml   MMI TRANSACCIONA 234,749 MC, TMP O/N 7,2707 P...   \n",
       "\n",
       "                                               TEXTO  \n",
       "0   LISBOA 12 Set (Reuter) - A inflação deverá re...  \n",
       "1   LISBOA, 30 Set (Reuter) - O Mercado Monetário...  \n",
       "2   LISBOA, 17 Out (Reuter) - As acções do Contín...  \n",
       "3   O escudo seguia relativamente estável na meia...  \n",
       "4   LISBOA, 20 Ago (Reuter) - O Mercado Monetário...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aplicando a função para o df de teste\n",
    "for idx in df_teste.index:\n",
    "    file = df_teste.at[idx, 'ID']\n",
    "    path_xml = f\"{path_news}\\{file}\"\n",
    "    titulo, texto = extract_xml_text(path_xml)\n",
    "    df_teste.at[idx, 'TITULO'] = titulo\n",
    "    df_teste.at[idx, 'TEXTO'] = texto\n",
    "\n",
    "df_teste = df_teste[['ID', 'TITULO', 'TEXTO']]\n",
    "df_teste.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratando os textos da base de dados\n",
    "- Aplicada a função de estemização para a linguagem dos textos (português)\n",
    "- Removendo os ascentos das palavras\n",
    "- Criando um limite de 200 temrmos por palavras, para evitar que a predição do classificador seja influenciada pelo tamanho da noticia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_portuguese(text, stemming = False, stopwords = False):\n",
    "    \"\"\"\n",
    "    Funcao usada para tratar textos escritos na lingua portuguesa\n",
    "\n",
    "    Parametros:\n",
    "        text: variavel do tipo string que contem o texto que devera ser tratado\n",
    "\n",
    "        stemming: variavel do tipo boolean que indica se a estemizacao deve ser aplicada ou nao\n",
    "\n",
    "        stopwords: variavel do tipo boolean que indica se as stopwords devem ser removidas ou nao\n",
    "    \"\"\"\n",
    "\n",
    "    # Lower case\n",
    "    text = text.lower()\n",
    "\n",
    "    # remove os acentos das palavras\n",
    "    nfkd_form = unicodedata.normalize('NFKD', text)\n",
    "    text = u\"\".join([c for c in nfkd_form if not unicodedata.combining(c)])\n",
    "\n",
    "    # remove tags HTML\n",
    "    regex = re.compile('<[^<>]+>')\n",
    "    text = re.sub(regex, \" \", text)\n",
    "\n",
    "    # normaliza as URLs\n",
    "    regex = re.compile('(http|https)://[^\\s]*')\n",
    "    text = re.sub(regex, \"<URL>\", text)\n",
    "\n",
    "    # normaliza emails\n",
    "    regex = re.compile('[^\\s]+@[^\\s]+')\n",
    "    text = re.sub(regex, \"<EMAIL>\", text)\n",
    "\n",
    "    # converte todos os caracteres não-alfanuméricos em espaço\n",
    "    regex = re.compile('[^A-Za-z0-9]+')\n",
    "    text = re.sub(regex, \" \", text)\n",
    "\n",
    "    # normaliza os numeros\n",
    "    regex = re.compile('[0-9]+.[0-9]+')\n",
    "    text = re.sub(regex, \"NUMERO\", text)\n",
    "\n",
    "    # normaliza os numeros\n",
    "    regex = re.compile('[0-9]+,[0-9]+')\n",
    "    text = re.sub(regex, \"NUMERO\", text)\n",
    "\n",
    "    # normaliza os numeros\n",
    "    regex = re.compile('[0-9]+')\n",
    "    text = re.sub(regex, \"NUMERO\", text)\n",
    "\n",
    "\n",
    "    # substitui varios espaçamentos seguidos em um só\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    # separa o texto em palavras\n",
    "    words = text.split()\n",
    "\n",
    "    # trunca o texto para apenas 200 termos\n",
    "    words = words[0:200]\n",
    "\n",
    "    # remove stopwords\n",
    "    if stopwords:\n",
    "        words = text.split() # separa o texto em palavras\n",
    "        words = [w for w in words if not w in nltk.corpus.stopwords.words('portuguese')]\n",
    "        text = \" \".join( words )\n",
    "\n",
    "    # aplica estemização\n",
    "    if stemming:\n",
    "        stemmer_method = RSLPStemmer()\n",
    "        words = [ stemmer_method.stem(w) for w in words ]\n",
    "        text = \" \".join( words )\n",
    "\n",
    "    # remove palavras compostas por apenas um caracter\n",
    "    words = text.split() # separa o texto em palavras\n",
    "    words = [ w for w in words if len(w)>1 ]\n",
    "    text = \" \".join( words )\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar a função ao DataFrame de treino\n",
    "df_train['TEXTO'] = df_train['TEXTO'].apply(preprocessing_portuguese)\n",
    "\n",
    "# Aplicando a função no df de teste\n",
    "df_teste['TEXTO'] = df_teste['TEXTO'].apply(preprocessing_portuguese)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gerando a representação vetorial\n",
    "\n",
    "Iremos transformar o texto em um vetor de atributos com valores numéricos. Uma das formas de fazer isso é considerar que cada palavra (ou token) da base de dados de treinamento é um atributo que armazena o número de vezes que uma determinada palavra aparece no texto. Na biblioteca `scikit-learn` podemos fazer essa conversão de texto para um vetor de atributos usando a função `skl.feature_extraction.text.CountVectorizer()`. Essa função gera um modelo de vetorização que pode ser ajustado com a base nos dados de treinamento usando a função `fit_transform()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 primeiras palavras do vocabulário obtidas a partir dos dados de treinamento:\n",
      "\n",
      "['aa' 'aaa' 'aanumero' 'aas' 'abaixo' 'abaixos' 'abaixou' 'abalada'\n",
      " 'abalaram' 'abanadas' 'abanar' 'abandona' 'abandonado' 'abandonaram'\n",
      " 'abastecimento' 'abatidas' 'abatimentos' 'abbey' 'abel' 'aberta']\n",
      "\n",
      "Dimensão dos dados vetorizados:  (2781, 10397)\n",
      "\n",
      "Dimensão dos dados vetorizados:  (1193, 10397)\n"
     ]
    }
   ],
   "source": [
    "# inicializa o modelo usado para gerar a representação TF (term frequency)\n",
    "vectorizer = skl.feature_extraction.text.CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None,\n",
    "                                                         stop_words = None, lowercase = True, binary=False, dtype=np.int32)\n",
    "\n",
    "# treina o modelo TF com os dados de treinamento e converte os dados de treinamento para uma array que contém a frequência dos termos em cada documento (TF - term frequency)\n",
    "X_train_tf = vectorizer.fit_transform(df_train['TEXTO'])\n",
    "\n",
    "# converte os dados de teste\n",
    "X_test_tf = vectorizer.transform(df_teste['TEXTO'])\n",
    "\n",
    "\n",
    "print('20 primeiras palavras do vocabulário obtidas a partir dos dados de treinamento:\\n')\n",
    "print(vectorizer.get_feature_names_out()[0:20])\n",
    "\n",
    "print('\\nDimensão dos dados vetorizados: ', X_train_tf.shape)\n",
    "print('\\nDimensão dos dados vetorizados: ', X_test_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2781, 10397)\n",
      "(1193, 10397)\n"
     ]
    }
   ],
   "source": [
    "X_train_bin = X_train_tf.copy()\n",
    "X_test_bin = X_test_tf.copy()\n",
    "\n",
    "#convert os dados para representação binária\n",
    "X_train_bin[X_train_bin!=0]=1\n",
    "\n",
    "#convert os dados para representação binária\n",
    "X_test_bin[X_test_bin!=0]=1 \n",
    "\n",
    "print(X_train_bin.shape)\n",
    "print(X_test_bin.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2781, 10397)\n",
      "(1193, 10397)\n"
     ]
    }
   ],
   "source": [
    "tfidf_model = skl.feature_extraction.text.TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
    "\n",
    "X_train_tfidf = tfidf_model.fit_transform(X_train_tf)\n",
    "X_test_tfidf = tfidf_model.transform(X_test_tf)\n",
    "\n",
    "print(X_train_tfidf.shape)\n",
    "print(X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "20 primeiras palavras da primeira amostra de treino\n",
      "['futuro', 'bdpotzNUMERO', 'dez', 'NUMERO', 'NUMERO', 'vs', 'NUMERO', 'no', 'fecho', 'anterior', 'futuro', 'bono', 'dez', 'NUMERO', 'mffzNUMERO', 'NUMERO', 'vs', 'NUMERO', 'no', 'fecho', 'anterior', 'lisboa', 'editorial', 'NUMERO', 'NUMERO', 'reuters', 'limited', 'NUMERO']\n",
      "\n",
      "\n",
      "20 primeiras palavras da primeira amostra de teste\n",
      "['lisboa', 'NUMERO', 'set', 'reuter', 'inflacao', 'devera', 'retomar', 'uma', 'trajectoria', 'descendente', 'logo', 'que', 'cessem', 'os', 'comportamentos', 'anomalos', 'de', 'alguns', 'bens', 'alimentares', 'refere', 'instituto', 'nacional', 'de', 'estatistica', 'ine', 'na', 'sua', 'sintese', 'mensal']\n"
     ]
    }
   ],
   "source": [
    "dataset2_train = []\n",
    "for i, msg in enumerate(df_train['TEXTO']):\n",
    "    dataset2_train.append(msg.split())\n",
    "\n",
    "dataset2_test = []\n",
    "for i, msg in enumerate(df_teste['TEXTO']):\n",
    "    dataset2_test.append(msg.split())\n",
    "\n",
    "print(\"\\n\\n20 primeiras palavras da primeira amostra de treino\")\n",
    "print(dataset2_train[0][0:30])\n",
    "\n",
    "print(\"\\n\\n20 primeiras palavras da primeira amostra de teste\")\n",
    "print(dataset2_test[0][0:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tamanho do vocabulário do modelo:  10398\n"
     ]
    }
   ],
   "source": [
    "sentencasEmbedding = None\n",
    "sentencasEmbedding = dataset2_train\n",
    "\n",
    "embeddingModel = Word2Vec(sentences = sentencasEmbedding,\n",
    "                          vector_size = 200,\n",
    "                          window = 3,\n",
    "                          min_count = 1)\n",
    "\n",
    "vocabSize = len(embeddingModel.wv)\n",
    "\n",
    "print(\"\\nTamanho do vocabulário do modelo: \", vocabSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDocvector(model, doc):\n",
    "    \"\"\"\n",
    "    obtem o vetor de cada palavra de um documento e calcula um vetor medio\n",
    "    \"\"\"\n",
    "\n",
    "    wordList = []\n",
    "    for word in doc:\n",
    "\n",
    "        try:\n",
    "            vec = model.wv[word]\n",
    "            wordList.append(vec)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    if len(wordList)>0:\n",
    "        vetorMedio = np.mean( wordList, axis=0 )\n",
    "    else:\n",
    "        vetorMedio = np.zeros( model.wv.vector_size )\n",
    "\n",
    "    return vetorMedio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset2featureMatrix(dataset, embeddingModel):\n",
    "\n",
    "    X_embedding = []\n",
    "    for doc in dataset:\n",
    "        vec = getDocvector(embeddingModel, doc)\n",
    "        X_embedding.append(vec)\n",
    "\n",
    "    X_embedding = np.array(X_embedding)\n",
    "\n",
    "    return X_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2781, 200)\n",
      "(1193, 200)\n"
     ]
    }
   ],
   "source": [
    "X_train_embedding = dataset2featureMatrix(dataset2_train, embeddingModel)\n",
    "X_test_embedding = dataset2featureMatrix(dataset2_test, embeddingModel)\n",
    "\n",
    "print(X_train_embedding.shape)\n",
    "print(X_test_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse\n",
    "\n",
    "# variável que deverá recer o modelo\n",
    "model = None\n",
    "\n",
    "# importa o modelo do scikitlearn\n",
    "import sklearn as skl\n",
    "from sklearn import linear_model\n",
    "\n",
    "def classificar(X_train, X_test, Y_train, Y_test):\n",
    "\n",
    "    ########################## COMPLETE O CÓDIGO AQUI  ########################\n",
    "\n",
    "    # inicia o classificador\n",
    "    model = skl.linear_model.LogisticRegression(C=0.5, max_iter = 500,\n",
    "                                               random_state = 10)\n",
    "\n",
    "    # normaliza\n",
    "    if not scipy.sparse.issparse(X_train):\n",
    "        scaler = skl.preprocessing.StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "    # treina o classificador com os dados de treinameto\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    # treina o classificador com os dados de treinameto\n",
    "    Y_pred = model.predict(X_test)\n",
    "\n",
    "    # obtem as medidas de desempenho\n",
    "    resultados = skl.metrics.classification_report(Y_test, Y_pred)\n",
    "\n",
    "    print(resultados)\n",
    "\n",
    "    ##########################################################################\n",
    "\n",
    "    return model\n",
    "\n",
    "print(\"\\n\\nTreinando com o formato TF\")\n",
    "model_tf = classificar(X_train_tf, X_test_tf, Y_train, Y_test)\n",
    "\n",
    "print(\"\\n\\nTreinando com o formato binário\")\n",
    "model_bin = classificar(X_train_bin, X_test_bin, Y_train, Y_test)\n",
    "\n",
    "print(\"\\n\\nTreinando com o formato TF-IDF\")\n",
    "model_tfidf = classificar(X_train_tfidf, X_test_tfidf, Y_train, Y_test)\n",
    "\n",
    "print(\"\\n\\nTreinando com word embeddings\")\n",
    "model_embedding = classificar(X_train_embedding, X_test_embedding, Y_train, Y_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
