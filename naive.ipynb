{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabriel-arrvda/af_inteligencia_artificial/blob/main/naive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "0yGWmgmRyYt2"
      },
      "outputs": [],
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "from sklearn import feature_extraction\n",
        "import sklearn as skl\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from nltk.stem import RSLPStemmer\n",
        "import nltk\n",
        "from zipfile import ZipFile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import os\n",
        "import re\n",
        "import gensim\n",
        "import scipy.sparse\n",
        "from gensim.models import Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "IZUiPQ4nDnb1",
        "outputId": "1ad6ea56-de95-442c-d878-943d320cbfbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/af_ia/\n",
        "\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TgZLHFtGL0v",
        "outputId": "638be228-d18e-4229-eeb8-e90f39b78b5e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/af_ia\n",
            "db  files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0RAebzj8yYt4"
      },
      "outputs": [],
      "source": [
        "# Caminho dos arquivos extraidos do kaggle\n",
        "path_dataset = r'files/classificao-de-notcias.zip'\n",
        "path_db = r'db'\n",
        "\n",
        "# Caminho dos arquivos que serão utilizados para a atividade\n",
        "path_train = r'db/arquivos_competicao/arquivos_competicao/train.csv'\n",
        "path_test = r'db/arquivos_competicao/arquivos_competicao/test.csv'\n",
        "path_news = r'db/arquivos_competicao/arquivos_competicao/news'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "l1-4D7HYyYt5",
        "outputId": "a53c5468-6fcd-409c-e5d5-7785af3f78e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arquivo já descompactado\n"
          ]
        }
      ],
      "source": [
        "def unzip(path, pathFolder):\n",
        "\n",
        "    # descompacta a base de dados de notícias\n",
        "    z = ZipFile(path, 'r')\n",
        "\n",
        "    if os.path.isdir(pathFolder):\n",
        "        z.extractall(pathFolder)\n",
        "        z.close()\n",
        "    else:\n",
        "        os.mkdir(pathFolder)\n",
        "        z.extractall(pathFolder)\n",
        "        z.close()\n",
        "\n",
        "    print(\"Arquivo descompactado com sucesso!\")\n",
        "\n",
        "# Antes de descompactar os arquivos valida se ja foram descompactados antes\n",
        "if not os.path.isdir(path_news):\n",
        "    unzip(path_dataset, path_db)\n",
        "else:\n",
        "    print(\"Arquivo já descompactado\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDa1FYtuyYt6"
      },
      "source": [
        "---\n",
        "## 1. Carregando os arquivos de teste e treino"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "sZDczMkqyYt7",
        "outputId": "58dcbc58-b80c-4dc4-e8fc-e2c123fdad05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colunas do arquivo de treino:  Index(['ID', 'Class'], dtype='object')\n",
            "Quantidade de linhas:  2781\n"
          ]
        }
      ],
      "source": [
        "#Carregando os arquivos de treino\n",
        "df_train = pd.read_csv(path_train)\n",
        "print(\"Colunas do arquivo de treino: \", df_train.columns)\n",
        "print(\"Quantidade de linhas: \", df_train.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Nxifz6vcyYt8",
        "outputId": "f074551f-32d2-4ae7-caf9-12a45f790b58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colunas do arquivo de treino:  Index(['ID'], dtype='object')\n",
            "Quantidade de linhas:  1193\n"
          ]
        }
      ],
      "source": [
        "# Carregando os arquivos de teste\n",
        "df_teste = pd.read_csv(path_test)\n",
        "print(\"Colunas do arquivo de treino: \", df_teste.columns)\n",
        "print(\"Quantidade de linhas: \", df_teste.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C99M5z45yYt8"
      },
      "source": [
        "---\n",
        "## Pré-Processamento dos Dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9V-Yv4mMyYt8"
      },
      "source": [
        "\n",
        "### Criando um DataFrame com os textos e titulos extraidos do XML\n",
        "Para facilitar a aplicação dos metodos foi adicionando novas colunas no DataFrame de treino e teste, contendo o texto e titulo extraidos dos arquivos xml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "oiHNPh4HyYt8"
      },
      "outputs": [],
      "source": [
        "def extract_xml_text(path_xml):\n",
        "\n",
        "    \"\"\"\n",
        "    A função `extract_xml_text` é designada para extrair os textos dos arquivos XML\n",
        "    especificados pelo `path_xml`.\n",
        "    Utilizando a biblioteca ElementTree para leitura do arquivo XML\n",
        "    \"\"\"\n",
        "\n",
        "    # Instancia um objeto como uma árvore de análise\n",
        "    tree = ET.parse(path_xml)\n",
        "\n",
        "    # Obtem o elemento raiz da árvore de ánalise\n",
        "    root = tree.getroot()\n",
        "\n",
        "    # Encontra o elemento headline (titulo) dentro da árvore de analise\n",
        "    headline = root.find('headline').text if root.find('headline') is not None else ''\n",
        "\n",
        "    # Entroa todos os elementos <p> que na estrutura dos xml's contem o texto\n",
        "    paragraphs = root.findall('.//p')\n",
        "\n",
        "    # Junta em uma unica string, separando por espaços\n",
        "    text = ' '.join([p.text for p in paragraphs if p.text is not None])\n",
        "\n",
        "    return headline, text\n",
        "\n",
        "def apply_extraction(df_applyed):\n",
        "\n",
        "    \"\"\"\n",
        "    Essa função é responsável em aplicar as novas colunas\n",
        "    no df_applyed passado como parametro.\n",
        "    \"\"\"\n",
        "\n",
        "    # Loop pelas linhas do df\n",
        "    for idx in df_applyed.index:\n",
        "\n",
        "        # atribui o valor da coluna id na variavel file\n",
        "        file = df_applyed.at[idx, 'ID']\n",
        "\n",
        "        # Concatena o nome do arquivo com o caminho dele\n",
        "        path_xml = f\"{path_news}/{file}\"\n",
        "\n",
        "        # Extrai o texto e titulo desse arquivo\n",
        "        titulo, texto = extract_xml_text(path_xml)\n",
        "\n",
        "        #Atribui esses o texto e titulos em novas colunas\n",
        "        df_applyed.at[idx, 'TITULO'] = titulo\n",
        "        df_applyed.at[idx, 'TEXTO'] = texto\n",
        "\n",
        "    return df_applyed\n",
        "\n",
        "def print_porcentagem(target):\n",
        "    # Calcula a contagem de cada classe\n",
        "    class_counts = target['Class'].value_counts()\n",
        "\n",
        "    # Calcula a porcentagem de cada classe\n",
        "    class_percentages = class_counts / len(target) * 100\n",
        "\n",
        "    # Imprime a porcentagem de cada classe\n",
        "    for cl, pct in class_percentages.items():\n",
        "        print(f\"Porcentagem da classe {cl}: {round(pct, 2)}%\")\n",
        "\n",
        "def preprocessing_portuguese(text, stemming = False, stopwords = False):\n",
        "    \"\"\"\n",
        "    Funcao usada para tratar textos escritos na lingua portuguesa\n",
        "\n",
        "    Parametros:\n",
        "        text: variavel do tipo string que contem o texto que devera ser tratado\n",
        "\n",
        "        stemming: variavel do tipo boolean que indica se a estemizacao deve ser aplicada ou nao\n",
        "\n",
        "        stopwords: variavel do tipo boolean que indica se as stopwords devem ser removidas ou nao\n",
        "    \"\"\"\n",
        "\n",
        "    # Lower case\n",
        "    text = text.lower()\n",
        "\n",
        "    # remove os acentos das palavras\n",
        "    nfkd_form = unicodedata.normalize('NFKD', text)\n",
        "    text = u\"\".join([c for c in nfkd_form if not unicodedata.combining(c)])\n",
        "\n",
        "    # remove tags HTML\n",
        "    regex = re.compile('<[^<>]+>')\n",
        "    text = re.sub(regex, \" \", text)\n",
        "\n",
        "    # normaliza as URLs\n",
        "    regex = re.compile('(http|https)://[^\\s]*')\n",
        "    text = re.sub(regex, \"<URL>\", text)\n",
        "\n",
        "    # normaliza emails\n",
        "    regex = re.compile('[^\\s]+@[^\\s]+')\n",
        "    text = re.sub(regex, \"<EMAIL>\", text)\n",
        "\n",
        "    # converte todos os caracteres não-alfanuméricos em espaço\n",
        "    regex = re.compile('[^A-Za-z0-9]+')\n",
        "    text = re.sub(regex, \" \", text)\n",
        "\n",
        "    # normaliza os numeros\n",
        "    regex = re.compile('[0-9]+.[0-9]+')\n",
        "    text = re.sub(regex, \"NUMERO\", text)\n",
        "\n",
        "    # normaliza os numeros\n",
        "    regex = re.compile('[0-9]+,[0-9]+')\n",
        "    text = re.sub(regex, \"NUMERO\", text)\n",
        "\n",
        "    # normaliza os numeros\n",
        "    regex = re.compile('[0-9]+')\n",
        "    text = re.sub(regex, \"NUMERO\", text)\n",
        "\n",
        "\n",
        "    # substitui varios espaçamentos seguidos em um só\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    # separa o texto em palavras\n",
        "    words = text.split()\n",
        "\n",
        "    # trunca o texto para apenas 200 termos\n",
        "    words = words[0:200]\n",
        "\n",
        "    # remove stopwords\n",
        "    if stopwords:\n",
        "        words = text.split() # separa o texto em palavras\n",
        "        words = [w for w in words if not w in nltk.corpus.stopwords.words('portuguese')]\n",
        "        text = \" \".join( words )\n",
        "\n",
        "    # aplica estemização\n",
        "    if stemming:\n",
        "        stemmer_method = RSLPStemmer()\n",
        "        words = [ stemmer_method.stem(w) for w in words ]\n",
        "        text = \" \".join( words )\n",
        "\n",
        "    # remove palavras compostas por apenas um caracter\n",
        "    words = text.split() # separa o texto em palavras\n",
        "    words = [ w for w in words if len(w)>1 ]\n",
        "    text = \" \".join( words )\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "N_FiAzU-yYt9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "263b6e81-f726-4b04-ead2-f3afd75ee0b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Porcentagem da classe Mercados: 74.33%\n",
            "Porcentagem da classe Economia: 21.18%\n",
            "Porcentagem da classe GovSocial: 3.24%\n",
            "Porcentagem da classe CorpIndustrial: 1.26%\n"
          ]
        }
      ],
      "source": [
        "df_train = apply_extraction(df_train)\n",
        "df_train = df_train[['ID', 'TITULO', 'TEXTO', 'Class']]\n",
        "print_porcentagem(df_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7hycTR3pyYt-"
      },
      "outputs": [],
      "source": [
        "df_teste = apply_extraction(df_teste)\n",
        "df_teste = df_teste[['ID', 'TITULO', 'TEXTO']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MM31fmfyYt-"
      },
      "source": [
        "### Tratando os textos da base de dados\n",
        "- Aplicada a função de estemização para a linguagem dos textos (português)\n",
        "- Removendo os ascentos das palavras\n",
        "- Criando um limite de 200 temrmos por palavras, para evitar que a predição do classificador seja influenciada pelo tamanho da noticia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Bd8939tcyYt_",
        "outputId": "d7e3c945-4054-4cd2-bb6a-c5e941605987",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Unzipping stemmers/rslp.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# Download the stopwords corpus\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Download the RSLPStemmer\n",
        "nltk.download('rslp')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "eJJxbTBlyYt_"
      },
      "outputs": [],
      "source": [
        "# Aplicar a função ao DataFrame de treino\n",
        "df_train['TEXTO'] = df_train['TEXTO'].apply(preprocessing_portuguese)\n",
        "\n",
        "# Aplicando a função no df de teste\n",
        "df_teste['TEXTO'] = df_teste['TEXTO'].apply(preprocessing_portuguese)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EumSz2CAyYt_"
      },
      "source": [
        "### Fazendo a divisão entre os dados de teste e treino"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "6emovbfvyYuA",
        "outputId": "f35b24f9-39c9-4d3d-cc42-c19cf3ba0071",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Qtd. dados de treinamento: 2224 (79.97%)\n",
            "Qtd. de dados de teste: 557 (20.03%)\n"
          ]
        }
      ],
      "source": [
        "# gera uma divisão dos dados em treino e teste\n",
        "cv = skl.model_selection.StratifiedShuffleSplit(n_splits=1, train_size=0.8,\n",
        "                                                random_state=10)\n",
        "\n",
        "# retorna os índices de treino e teste\n",
        "dataset = df_train['TEXTO']\n",
        "target = df_train['Class']\n",
        "train_index, test_index = list( cv.split(dataset, target) )[0]\n",
        "\n",
        "# retorna as partições de treino e teste de acordo com os índices\n",
        "dataset_train, dataset_test = dataset[train_index], dataset[test_index]\n",
        "Y_train, Y_test = target[train_index], target[test_index]\n",
        "\n",
        "print('Qtd. dados de treinamento: %d (%1.2f%%)' %(dataset_train.shape[0], (dataset_train.shape[0]/dataset.shape[0])*100) )\n",
        "print('Qtd. de dados de teste: %d (%1.2f%%)' %(dataset_test.shape[0], (dataset_test.shape[0]/dataset.shape[0])*100) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVG402s3yYuA"
      },
      "source": [
        "### Gerando a representação vetorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIqy8VyRyYuA"
      },
      "source": [
        "Gerando a representação vetorial\n",
        "\n",
        "Iremos transformar o texto em um vetor de atributos com valores numéricos. Uma das formas de fazer isso é considerar que cada palavra (ou token) da base de dados de treinamento é um atributo que armazena o número de vezes que uma determinada palavra aparece no texto. Na biblioteca `scikit-learn` podemos fazer essa conversão de texto para um vetor de atributos usando a função `skl.feature_extraction.text.CountVectorizer()`. Essa função gera um modelo de vetorização que pode ser ajustado com a base nos dados de treinamento usando a função `fit_transform()`.\n",
        "\n",
        "Obs.: você deve treinar o modelo de representação **apenas com os dados de treinamento**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "V7As4k03yYuA",
        "outputId": "03fb49e4-2aa2-4f4c-d0c3-bb791554924d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20 primeiras palavras do vocabulário obtidas a partir dos dados de treinamento:\n",
            "\n",
            "['aa' 'aaa' 'aanumero' 'aas' 'abaixo' 'abaixos' 'abaixou' 'abalada'\n",
            " 'abanadas' 'abanar' 'abandona' 'abandonado' 'abandonaram' 'abastecimento'\n",
            " 'abatidas' 'abatimentos' 'aberta' 'abertamente' 'abertas' 'aberto']\n",
            "\n",
            "Dimensão dos dados vetorizados:  (2224, 9506)\n",
            "\n",
            "Dimensão dos dados vetorizados:  (557, 9506)\n"
          ]
        }
      ],
      "source": [
        "# inicializa o modelo usado para gerar a representação TF (term frequency)\n",
        "vectorizer = skl.feature_extraction.text.CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None,\n",
        "                                                         stop_words = None, lowercase = True, binary=False, dtype=np.int32)\n",
        "\n",
        "# treina o modelo TF com os dados de treinamento e converte os dados de treinamento para uma array que contém a frequência dos termos em cada documento (TF - term frequency)\n",
        "X_train_tf = vectorizer.fit_transform(dataset_train)\n",
        "\n",
        "# converte os dados de teste\n",
        "X_test_tf = vectorizer.transform(dataset_test)\n",
        "\n",
        "print('20 primeiras palavras do vocabulário obtidas a partir dos dados de treinamento:\\n')\n",
        "print(vectorizer.get_feature_names_out()[0:20])\n",
        "\n",
        "print('\\nDimensão dos dados vetorizados: ', X_train_tf.shape)\n",
        "print('\\nDimensão dos dados vetorizados: ', X_test_tf.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8h1ysItyYuB"
      },
      "source": [
        "\n",
        "## Gerando word embeddings\n",
        "\n",
        "Depois de fazer o pré-processamento, é necessário transformar o texto em um vetor de atributos com valores numéricos. Podemos fazer isso usando word embeddings pré-treinadas ou treinando um modelo próprio.\n",
        "\n",
        "Vamos passar por toda a base de dados e transformar cada documento em uma lista de palavra."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "yEBJocaByYuB",
        "outputId": "abc4174c-82b4-4b87-f0fb-52e9d1522b85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "20 primeiras palavras da primeira amostra de treino\n",
            "['lisboa', 'NUMERO', 'mai', 'reuter', 'indice', 'de', 'precos', 'dos', 'bens', 'nao', 'transaccionaveis', 'foi', 'em', 'abril', 'de', 'NUMERO', 'pct', 'contra', 'NUMERO', 'pct', 'no', 'mes', 'de', 'marco', 'anunciou', 'hoje', 'instituto', 'nacional', 'de', 'estatistica']\n",
            "\n",
            "\n",
            "20 primeiras palavras da primeira amostra de teste\n",
            "['lisboa', 'NUMERO', 'jun', 'reuter', 'indice', 'de', 'precos', 'no', 'consumidor', 'ipc', 'devera', 'registar', 'uma', 'evolucao', 'moderada', 'apesar', 'de', 'em', 'maio', 'ipc', 'homologo', 'ter', 'invertido', 'tendencia', 'descendente', 'dos', 'meses', 'anteriores', 'refere', 'sintese']\n"
          ]
        }
      ],
      "source": [
        "dataset2_train = []\n",
        "for i, msg in enumerate(dataset_train):\n",
        "    dataset2_train.append(msg.split())\n",
        "\n",
        "dataset2_test = []\n",
        "for i, msg in enumerate(dataset_test):\n",
        "    dataset2_test.append(msg.split())\n",
        "\n",
        "print(\"\\n\\n20 primeiras palavras da primeira amostra de treino\")\n",
        "print(dataset2_train[0][0:30])\n",
        "\n",
        "print(\"\\n\\n20 primeiras palavras da primeira amostra de teste\")\n",
        "print(dataset2_test[0][0:30])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykcNNMfByYuB"
      },
      "source": [
        "Neste momento, vamos treinar modelo próprio de embeddings baseado nos dados de treinamento. Para manipular as embeddings, iremos usar a biblioteca Gensim: https://radimrehurek.com/gensim/models/word2vec.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "40w8HdNtyYuB",
        "outputId": "ee5a5220-d6cd-429b-d3c9-f748067d38b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tamanho do vocabulário do modelo:  9507\n"
          ]
        }
      ],
      "source": [
        "sentencasEmbedding = dataset2_train\n",
        "embeddingModel = Word2Vec(sentences = sentencasEmbedding,\n",
        "                          vector_size = 200,\n",
        "                          window = 3,\n",
        "                          min_count = 1)\n",
        "\n",
        "vocabSize = len(embeddingModel.wv)\n",
        "\n",
        "print(\"\\nTamanho do vocabulário do modelo: \", vocabSize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeuLhvpcyYuC"
      },
      "source": [
        "### Gerando os vetores para todos os documentos da base de dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "PZUwlgGgyYuC"
      },
      "outputs": [],
      "source": [
        "def getDocvector(model, doc):\n",
        "    \"\"\"\n",
        "    obtem o vetor de cada palavra de um documento e calcula um vetor medio\n",
        "    \"\"\"\n",
        "\n",
        "    wordList = []\n",
        "    for word in doc:\n",
        "\n",
        "        try:\n",
        "            vec = model.wv[word]\n",
        "            wordList.append(vec)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    if len(wordList)>0:\n",
        "        vetorMedio = np.mean( wordList, axis=0 )\n",
        "    else:\n",
        "        vetorMedio = np.zeros( model.wv.vector_size )\n",
        "\n",
        "    return vetorMedio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "pOdz4sZGyYuC",
        "outputId": "a48e0b1e-6db3-4de4-8387-fd59d8ce8671",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2224, 200)\n",
            "(557, 200)\n"
          ]
        }
      ],
      "source": [
        "def dataset2featureMatrix(dataset, embeddingModel):\n",
        "\n",
        "    X_embedding = []\n",
        "    for doc in dataset:\n",
        "        vec = getDocvector(embeddingModel, doc)\n",
        "        X_embedding.append(vec)\n",
        "\n",
        "    X_embedding = np.array(X_embedding)\n",
        "    return X_embedding\n",
        "\n",
        "\n",
        "X_train_embedding = dataset2featureMatrix(dataset2_train, embeddingModel)\n",
        "X_test_embedding = dataset2featureMatrix(dataset2_test, embeddingModel)\n",
        "\n",
        "print(X_train_embedding.shape)\n",
        "print(X_test_embedding.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6MKEqRiyYuC"
      },
      "source": [
        "_______\n",
        "# Treinando um modelo de classificação por Naive Bayes Multinomial\n",
        "\n",
        "Utilizando os vetores gerados a partir de TF e word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "OmAgnxBJyYuD"
      },
      "outputs": [],
      "source": [
        "def classifica(X_train, X_test,\n",
        "                Y_train, Y_test,\n",
        "                formato = \"TF\"):\n",
        "\n",
        "  model = MultinomialNB(\n",
        "      alpha=0.8, fit_prior=True, force_alpha=True\n",
        "  )\n",
        "\n",
        "  # normaliza os dados\n",
        "  if formato==\"embedding\":\n",
        "    scaler = skl.preprocessing.MinMaxScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "  elif formato==\"TF\":\n",
        "    scaler = skl.preprocessing.Normalizer(norm='l2')\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "  # treinando o método de classificação\n",
        "  model.fit( X_train, Y_train )\n",
        "\n",
        "  # classifica os dados de teste\n",
        "  Y_pred = model.predict(X_test)\n",
        "\n",
        "  # calcula as métricas de desempenho\n",
        "  res = skl.metrics.classification_report(\n",
        "      Y_test, Y_pred)\n",
        "\n",
        "  print(res)\n",
        "  print(\"\\nAcurácia: \", skl.metrics.accuracy_score(Y_test, Y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Classificação TF\")\n",
        "classifica(X_train_tf, X_test_tf, Y_train, Y_test,\n",
        "            formato = \"TF\")\n",
        "\n",
        "\n",
        "print(\"\\nClassificação word embbeddings\")\n",
        "classifica(X_train_embedding, X_test_embedding, Y_train, Y_test,\n",
        "            formato = \"embedding\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoZ9uwmxOxaV",
        "outputId": "c2d02a18-fe82-4558-f7e2-3934cbcb7ee8"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classificação TF\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "CorpIndustrial       0.00      0.00      0.00         7\n",
            "      Economia       0.65      0.11      0.19       118\n",
            "     GovSocial       0.00      0.00      0.00        18\n",
            "      Mercados       0.77      1.00      0.87       414\n",
            "\n",
            "      accuracy                           0.76       557\n",
            "     macro avg       0.35      0.28      0.26       557\n",
            "  weighted avg       0.71      0.76      0.69       557\n",
            "\n",
            "\n",
            "Acurácia:  0.7648114901256733\n",
            "\n",
            "Classificação word embbeddings\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "CorpIndustrial       0.00      0.00      0.00         7\n",
            "      Economia       0.53      0.59      0.56       118\n",
            "     GovSocial       0.30      0.89      0.44        18\n",
            "      Mercados       0.94      0.84      0.89       414\n",
            "\n",
            "      accuracy                           0.78       557\n",
            "     macro avg       0.44      0.58      0.47       557\n",
            "  weighted avg       0.82      0.78      0.79       557\n",
            "\n",
            "\n",
            "Acurácia:  0.7791741472172352\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}