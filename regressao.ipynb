{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "0yGWmgmRyYt2"
      },
      "outputs": [],
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "from sklearn import feature_extraction\n",
        "import sklearn as skl\n",
        "from nltk.stem import RSLPStemmer\n",
        "import nltk\n",
        "from zipfile import ZipFile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import os\n",
        "import re\n",
        "import gensim\n",
        "import scipy.sparse\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.metrics import roc_auc_score, make_scorer\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForPreTraining\n",
        "from transformers import AutoModel\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import sklearn.preprocessing as skl_preprocessing\n",
        "import sklearn.metrics as skl_metrics\n",
        "from sklearn import model_selection\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import model_selection\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import roc_auc_score, make_scorer\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForPreTraining\n",
        "from transformers import AutoModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZUiPQ4nDnb1",
        "outputId": "5aa17c5e-d2da-4191-a7f8-903836cac2f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TgZLHFtGL0v",
        "outputId": "66b2594d-8687-4da3-fd29-8de2fd329301"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/af_ia\n",
            "db  files\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/af_ia/\n",
        "\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0RAebzj8yYt4"
      },
      "outputs": [],
      "source": [
        "# Caminho dos arquivos extraidos do kaggle\n",
        "path_dataset = r'files/classificao-de-notcias.zip'\n",
        "path_db = r'db'\n",
        "\n",
        "# Caminho dos arquivos que serão utilizados para a atividade\n",
        "path_train = r'db/arquivos_competicao/arquivos_competicao/train.csv'\n",
        "path_test = r'db/arquivos_competicao/arquivos_competicao/test.csv'\n",
        "path_news = r'db/arquivos_competicao/arquivos_competicao/news'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1-4D7HYyYt5",
        "outputId": "4a293a8e-6ef8-4aec-8846-a9fb140ab538"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arquivo já descompactado\n"
          ]
        }
      ],
      "source": [
        "def unzip(path, pathFolder):\n",
        "\n",
        "    # descompacta a base de dados de notícias\n",
        "    z = ZipFile(path, 'r')\n",
        "\n",
        "    if os.path.isdir(pathFolder):\n",
        "        z.extractall(pathFolder)\n",
        "        z.close()\n",
        "    else:\n",
        "        os.mkdir(pathFolder)\n",
        "        z.extractall(pathFolder)\n",
        "        z.close()\n",
        "\n",
        "    print(\"Arquivo descompactado com sucesso!\")\n",
        "\n",
        "# Antes de descompactar os arquivos valida se ja foram descompactados antes\n",
        "if not os.path.isdir(path_news):\n",
        "    unzip(path_dataset, path_db)\n",
        "else:\n",
        "    print(\"Arquivo já descompactado\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDa1FYtuyYt6"
      },
      "source": [
        "---\n",
        "## 1. Carregando os arquivos de teste e treino"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZDczMkqyYt7",
        "outputId": "3d4c1263-625b-4692-dc4c-8add5ef1361f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colunas do arquivo de treino:  Index(['ID', 'Class'], dtype='object')\n",
            "Quantidade de linhas:  2781\n"
          ]
        }
      ],
      "source": [
        "#Carregando os arquivos de treino\n",
        "df_train = pd.read_csv(path_train)\n",
        "print(\"Colunas do arquivo de treino: \", df_train.columns)\n",
        "print(\"Quantidade de linhas: \", df_train.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nxifz6vcyYt8",
        "outputId": "0d45dadc-6b4b-4409-f6a0-b0180291b060"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colunas do arquivo de treino:  Index(['ID'], dtype='object')\n",
            "Quantidade de linhas:  1193\n"
          ]
        }
      ],
      "source": [
        "# Carregando os arquivos de teste\n",
        "df_teste = pd.read_csv(path_test)\n",
        "print(\"Colunas do arquivo de treino: \", df_teste.columns)\n",
        "print(\"Quantidade de linhas: \", df_teste.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C99M5z45yYt8"
      },
      "source": [
        "---\n",
        "## Pré-Processamento dos Dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9V-Yv4mMyYt8"
      },
      "source": [
        "\n",
        "### Criando um DataFrame com os textos e titulos extraidos do XML\n",
        "Para facilitar a aplicação dos metodos foi adicionando novas colunas no DataFrame de treino e teste, contendo o texto e titulo extraidos dos arquivos xml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "oiHNPh4HyYt8"
      },
      "outputs": [],
      "source": [
        "def extract_xml_text(path_xml):\n",
        "\n",
        "    \"\"\"\n",
        "    A função `extract_xml_text` é designada para extrair os textos dos arquivos XML\n",
        "    especificados pelo `path_xml`.\n",
        "    Utilizando a biblioteca ElementTree para leitura do arquivo XML\n",
        "    \"\"\"\n",
        "\n",
        "    # Instancia um objeto como uma árvore de análise\n",
        "    tree = ET.parse(path_xml)\n",
        "\n",
        "    # Obtem o elemento raiz da árvore de ánalise\n",
        "    root = tree.getroot()\n",
        "\n",
        "    # Encontra o elemento headline (titulo) dentro da árvore de analise\n",
        "    headline = root.find('headline').text if root.find('headline') is not None else ''\n",
        "\n",
        "    # Entroa todos os elementos <p> que na estrutura dos xml's contem o texto\n",
        "    paragraphs = root.findall('.//p')\n",
        "\n",
        "    # Junta em uma unica string, separando por espaços\n",
        "    text = ' '.join([p.text for p in paragraphs if p.text is not None])\n",
        "\n",
        "    return headline, text\n",
        "\n",
        "def apply_extraction(df_applyed):\n",
        "\n",
        "    \"\"\"\n",
        "    Essa função é responsável em aplicar as novas colunas\n",
        "    no df_applyed passado como parametro.\n",
        "    \"\"\"\n",
        "\n",
        "    # Loop pelas linhas do df\n",
        "    for idx in df_applyed.index:\n",
        "\n",
        "        # atribui o valor da coluna id na variavel file\n",
        "        file = df_applyed.at[idx, 'ID']\n",
        "\n",
        "        # Concatena o nome do arquivo com o caminho dele\n",
        "        path_xml = f\"{path_news}/{file}\"\n",
        "\n",
        "        # Extrai o texto e titulo desse arquivo\n",
        "        titulo, texto = extract_xml_text(path_xml)\n",
        "\n",
        "        #Atribui esses o texto e titulos em novas colunas\n",
        "        df_applyed.at[idx, 'TITULO'] = titulo\n",
        "        df_applyed.at[idx, 'TEXTO'] = texto\n",
        "\n",
        "    return df_applyed\n",
        "\n",
        "def print_porcentagem(target):\n",
        "    # Calcula a contagem de cada classe\n",
        "    class_counts = target['Class'].value_counts()\n",
        "\n",
        "    # Calcula a porcentagem de cada classe\n",
        "    class_percentages = class_counts / len(target) * 100\n",
        "\n",
        "    # Imprime a porcentagem de cada classe\n",
        "    for cl, pct in class_percentages.items():\n",
        "        print(f\"Porcentagem da classe {cl}: {round(pct, 2)}%\")\n",
        "\n",
        "def preprocessing_portuguese(text, stemming = False, stopwords = False):\n",
        "    \"\"\"\n",
        "    Funcao usada para tratar textos escritos na lingua portuguesa\n",
        "\n",
        "    Parametros:\n",
        "        text: variavel do tipo string que contem o texto que devera ser tratado\n",
        "\n",
        "        stemming: variavel do tipo boolean que indica se a estemizacao deve ser aplicada ou nao\n",
        "\n",
        "        stopwords: variavel do tipo boolean que indica se as stopwords devem ser removidas ou nao\n",
        "    \"\"\"\n",
        "\n",
        "    # Lower case\n",
        "    text = text.lower()\n",
        "\n",
        "    # remove os acentos das palavras\n",
        "    nfkd_form = unicodedata.normalize('NFKD', text)\n",
        "    text = u\"\".join([c for c in nfkd_form if not unicodedata.combining(c)])\n",
        "\n",
        "    # remove tags HTML\n",
        "    regex = re.compile('<[^<>]+>')\n",
        "    text = re.sub(regex, \" \", text)\n",
        "\n",
        "    # normaliza as URLs\n",
        "    regex = re.compile('(http|https)://[^\\s]*')\n",
        "    text = re.sub(regex, \"<URL>\", text)\n",
        "\n",
        "    # normaliza emails\n",
        "    regex = re.compile('[^\\s]+@[^\\s]+')\n",
        "    text = re.sub(regex, \"<EMAIL>\", text)\n",
        "\n",
        "    # converte todos os caracteres não-alfanuméricos em espaço\n",
        "    regex = re.compile('[^A-Za-z0-9]+')\n",
        "    text = re.sub(regex, \" \", text)\n",
        "\n",
        "    # normaliza os numeros\n",
        "    regex = re.compile('[0-9]+.[0-9]+')\n",
        "    text = re.sub(regex, \"NUMERO\", text)\n",
        "\n",
        "    # normaliza os numeros\n",
        "    regex = re.compile('[0-9]+,[0-9]+')\n",
        "    text = re.sub(regex, \"NUMERO\", text)\n",
        "\n",
        "    # normaliza os numeros\n",
        "    regex = re.compile('[0-9]+')\n",
        "    text = re.sub(regex, \"NUMERO\", text)\n",
        "\n",
        "\n",
        "    # substitui varios espaçamentos seguidos em um só\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    # separa o texto em palavras\n",
        "    words = text.split()\n",
        "\n",
        "    # trunca o texto para apenas 200 termos\n",
        "    words = words[0:200]\n",
        "\n",
        "    # remove stopwords\n",
        "    if stopwords:\n",
        "        words = text.split() # separa o texto em palavras\n",
        "        words = [w for w in words if not w in nltk.corpus.stopwords.words('portuguese')]\n",
        "        text = \" \".join( words )\n",
        "\n",
        "    # aplica estemização\n",
        "    if stemming:\n",
        "        stemmer_method = RSLPStemmer()\n",
        "        words = [ stemmer_method.stem(w) for w in words ]\n",
        "        text = \" \".join( words )\n",
        "\n",
        "    # remove palavras compostas por apenas um caracter\n",
        "    words = text.split() # separa o texto em palavras\n",
        "    words = [ w for w in words if len(w)>1 ]\n",
        "    text = \" \".join( words )\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_FiAzU-yYt9",
        "outputId": "1db9b97c-e7da-4554-ec61-9b225c857abe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Porcentagem da classe Mercados: 74.33%\n",
            "Porcentagem da classe Economia: 21.18%\n",
            "Porcentagem da classe GovSocial: 3.24%\n",
            "Porcentagem da classe CorpIndustrial: 1.26%\n"
          ]
        }
      ],
      "source": [
        "df_train = apply_extraction(df_train)\n",
        "df_train = df_train[['ID', 'TITULO', 'TEXTO', 'Class']]\n",
        "print_porcentagem(df_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformação do target\n",
        "le = LabelEncoder()\n",
        "df_train['Class'] = le.fit_transform(df_train['Class'])"
      ],
      "metadata": {
        "id": "8Ft0fpU5Jx7R"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7hycTR3pyYt-"
      },
      "outputs": [],
      "source": [
        "df_teste = apply_extraction(df_teste)\n",
        "df_teste = df_teste[['ID', 'TITULO', 'TEXTO']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MM31fmfyYt-"
      },
      "source": [
        "### Tratando os textos da base de dados\n",
        "- Aplicada a função de estemização para a linguagem dos textos (português)\n",
        "- Removendo os ascentos das palavras\n",
        "- Criando um limite de 200 temrmos por palavras, para evitar que a predição do classificador seja influenciada pelo tamanho da noticia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Bd8939tcyYt_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4240b57e-57c1-471a-a9ec-6782a15c9750"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Unzipping stemmers/rslp.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# Download the stopwords corpus\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Download the RSLPStemmer\n",
        "nltk.download('rslp')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "eJJxbTBlyYt_"
      },
      "outputs": [],
      "source": [
        "# Aplicar a função ao DataFrame de treino\n",
        "df_train['TEXTO'] = df_train['TEXTO'].apply(preprocessing_portuguese)\n",
        "\n",
        "# Aplicando a função no df de teste\n",
        "df_teste['TEXTO'] = df_teste['TEXTO'].apply(preprocessing_portuguese)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EumSz2CAyYt_"
      },
      "source": [
        "### Fazendo a divisão entre os dados de teste e treino"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6emovbfvyYuA",
        "outputId": "3030fbb8-7cc2-48c8-80c6-9cf144667092"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Qtd. dados de treinamento: 2224 (79.97%)\n",
            "Qtd. de dados de teste: 557 (20.03%)\n"
          ]
        }
      ],
      "source": [
        "# gera uma divisão dos dados em treino e teste\n",
        "cv = skl.model_selection.StratifiedShuffleSplit(n_splits=1, train_size=0.8,\n",
        "                                                random_state=10)\n",
        "\n",
        "# retorna os índices de treino e teste\n",
        "dataset = df_train['TEXTO']\n",
        "target = df_train['Class']\n",
        "train_index, test_index = list( cv.split(dataset, target) )[0]\n",
        "\n",
        "# retorna as partições de treino e teste de acordo com os índices\n",
        "dataset_train, dataset_test = dataset[train_index], dataset[test_index]\n",
        "Y_train, Y_test = target[train_index], target[test_index]\n",
        "\n",
        "print('Qtd. dados de treinamento: %d (%1.2f%%)' %(dataset_train.shape[0], (dataset_train.shape[0]/dataset.shape[0])*100) )\n",
        "print('Qtd. de dados de teste: %d (%1.2f%%)' %(dataset_test.shape[0], (dataset_test.shape[0]/dataset.shape[0])*100) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVG402s3yYuA"
      },
      "source": [
        "### Gerando a representação vetorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIqy8VyRyYuA"
      },
      "source": [
        "#### Gerando a representação vetorial para TF\n",
        "\n",
        "Iremos transformar o texto em um vetor de atributos com valores numéricos. Uma das formas de fazer isso é considerar que cada palavra (ou token) da base de dados de treinamento é um atributo que armazena o número de vezes que uma determinada palavra aparece no texto. Na biblioteca `scikit-learn` podemos fazer essa conversão de texto para um vetor de atributos usando a função `skl.feature_extraction.text.CountVectorizer()`. Essa função gera um modelo de vetorização que pode ser ajustado com a base nos dados de treinamento usando a função `fit_transform()`.\n",
        "\n",
        "Obs.: você deve treinar o modelo de representação **apenas com os dados de treinamento**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7As4k03yYuA",
        "outputId": "f53d7e01-bed0-44a7-80fc-c1fb103a71f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20 primeiras palavras do vocabulário obtidas a partir dos dados de treinamento:\n",
            "\n",
            "['aa' 'aaa' 'aanumero' 'aas' 'abaixo' 'abaixos' 'abaixou' 'abalada'\n",
            " 'abanadas' 'abanar' 'abandona' 'abandonado' 'abandonaram' 'abastecimento'\n",
            " 'abatidas' 'abatimentos' 'aberta' 'abertamente' 'abertas' 'aberto']\n",
            "\n",
            "Dimensão dos dados vetorizados:  (2224, 9506)\n",
            "\n",
            "Dimensão dos dados vetorizados:  (557, 9506)\n"
          ]
        }
      ],
      "source": [
        "# inicializa o modelo usado para gerar a representação TF (term frequency)\n",
        "vectorizer = skl.feature_extraction.text.CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None,\n",
        "                                                         stop_words = None, lowercase = True, binary=False, dtype=np.int32)\n",
        "\n",
        "# treina o modelo TF com os dados de treinamento e converte os dados de treinamento para uma array que contém a frequência dos termos em cada documento (TF - term frequency)\n",
        "X_train_tf = vectorizer.fit_transform(dataset_train)\n",
        "\n",
        "# converte os dados de teste\n",
        "X_test_tf = vectorizer.transform(dataset_test)\n",
        "\n",
        "print('20 primeiras palavras do vocabulário obtidas a partir dos dados de treinamento:\\n')\n",
        "print(vectorizer.get_feature_names_out()[0:20])\n",
        "\n",
        "print('\\nDimensão dos dados vetorizados: ', X_train_tf.shape)\n",
        "print('\\nDimensão dos dados vetorizados: ', X_test_tf.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2s-w6Vvv2SG"
      },
      "source": [
        "#### Gerando a representação vetorial para binário\n",
        "\n",
        "Utilizando a técnica de vetorização binária onde cada termo é representado como um valor binário. Ou seja, se o termo aparece no documento o valor é 1, caso contrário o valor é 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6EJG3Erv9hQ",
        "outputId": "24dac5aa-0eca-4b1a-a33d-7160af059230"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2224, 9506)\n",
            "(557, 9506)\n"
          ]
        }
      ],
      "source": [
        "X_train_bin = X_train_tf.copy()\n",
        "X_test_bin = X_test_tf.copy()\n",
        "\n",
        "#convert os dados de treino para representação binária\n",
        "X_train_bin[X_train_bin!=0]=1\n",
        "\n",
        "#convert os dados de teste para representação binária\n",
        "X_test_bin[X_test_bin!=0]=1\n",
        "\n",
        "print(X_train_bin.shape)\n",
        "print(X_test_bin.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCactdlWwe6g"
      },
      "source": [
        "#### TF-IDF\n",
        "\n",
        "Utilizando a função **TfidfTransformer** para para converter a representação vetorial TF para TF-IDF\n",
        "Os parametros utilizados nessa função foram:\n",
        "- **norm=l2**: normalizando cada vetor TF-IDF para que a soma dos quadrados dos elementos seja igual a 1\n",
        "- **smooth_idf**: adiciona 1 ao denominador da formula de IDF para eviter divisoes por zero\n",
        "- **sublinear_tf**: aplica a sublinearização do TF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mSB6PHUweq3",
        "outputId": "c7abba5c-cb4f-4b81-c416-9c4cd1463f85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2224, 9506)\n",
            "(557, 9506)\n"
          ]
        }
      ],
      "source": [
        "#Inicializa o modelo usado para gerar a representação TF-IDF\n",
        "tfidf_model = skl.feature_extraction.text.TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
        "\n",
        "# Treina o modelo com os vetores de treinamento\n",
        "X_train_tfidf = tfidf_model.fit_transform(X_train_tf)\n",
        "\n",
        "# treina o modelo com os dados de teste\n",
        "X_test_tfidf = tfidf_model.transform(X_test_tf)\n",
        "\n",
        "print(X_train_tfidf.shape)\n",
        "print(X_test_tfidf.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1htD0BMw_GT"
      },
      "source": [
        "### Gerando word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8h1ysItyYuB"
      },
      "source": [
        "\n",
        "#### Gerando com a própria base\n",
        "\n",
        "Depois de fazer o pré-processamento, é necessário transformar o texto em um vetor de atributos com valores numéricos. Podemos fazer isso usando word embeddings pré-treinadas ou treinando um modelo próprio.\n",
        "\n",
        "Vamos passar por toda a base de dados e transformar cada documento em uma lista de palavra."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEBJocaByYuB",
        "outputId": "c15748a1-e14a-4250-9fb0-13a2f81323ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "20 primeiras palavras da primeira amostra de treino\n",
            "['lisboa', 'NUMERO', 'mai', 'reuter', 'indice', 'de', 'precos', 'dos', 'bens', 'nao', 'transaccionaveis', 'foi', 'em', 'abril', 'de', 'NUMERO', 'pct', 'contra', 'NUMERO', 'pct', 'no', 'mes', 'de', 'marco', 'anunciou', 'hoje', 'instituto', 'nacional', 'de', 'estatistica']\n",
            "\n",
            "\n",
            "20 primeiras palavras da primeira amostra de teste\n",
            "['lisboa', 'NUMERO', 'jun', 'reuter', 'indice', 'de', 'precos', 'no', 'consumidor', 'ipc', 'devera', 'registar', 'uma', 'evolucao', 'moderada', 'apesar', 'de', 'em', 'maio', 'ipc', 'homologo', 'ter', 'invertido', 'tendencia', 'descendente', 'dos', 'meses', 'anteriores', 'refere', 'sintese']\n"
          ]
        }
      ],
      "source": [
        "dataset2_train = []\n",
        "for i, msg in enumerate(dataset_train):\n",
        "    dataset2_train.append(msg.split())\n",
        "\n",
        "dataset2_test = []\n",
        "for i, msg in enumerate(dataset_test):\n",
        "    dataset2_test.append(msg.split())\n",
        "\n",
        "print(\"\\n\\n20 primeiras palavras da primeira amostra de treino\")\n",
        "print(dataset2_train[0][0:30])\n",
        "\n",
        "print(\"\\n\\n20 primeiras palavras da primeira amostra de teste\")\n",
        "print(dataset2_test[0][0:30])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykcNNMfByYuB"
      },
      "source": [
        "Neste momento, vamos treinar modelo próprio de embeddings baseado nos dados de treinamento. Para manipular as embeddings, iremos usar a biblioteca Gensim: https://radimrehurek.com/gensim/models/word2vec.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40w8HdNtyYuB",
        "outputId": "ae99da1d-2570-4691-88ea-819024151e52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tamanho do vocabulário do modelo:  9507\n"
          ]
        }
      ],
      "source": [
        "sentencasEmbedding = dataset2_train\n",
        "embeddingModel = Word2Vec(sentences = sentencasEmbedding,\n",
        "                          vector_size = 200,\n",
        "                          window = 3,\n",
        "                          min_count = 1)\n",
        "\n",
        "vocabSize = len(embeddingModel.wv)\n",
        "\n",
        "print(\"\\nTamanho do vocabulário do modelo: \", vocabSize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "PZUwlgGgyYuC"
      },
      "outputs": [],
      "source": [
        "def getDocvector(model, doc):\n",
        "    \"\"\"\n",
        "    obtem o vetor de cada palavra de um documento e calcula um vetor medio\n",
        "    \"\"\"\n",
        "\n",
        "    wordList = []\n",
        "    for word in doc:\n",
        "\n",
        "        try:\n",
        "            vec = model.wv[word]\n",
        "            wordList.append(vec)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    if len(wordList)>0:\n",
        "        vetorMedio = np.mean( wordList, axis=0 )\n",
        "    else:\n",
        "        vetorMedio = np.zeros( model.wv.vector_size )\n",
        "\n",
        "    return vetorMedio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOdz4sZGyYuC",
        "outputId": "e776aa90-0b2f-4f83-cd6b-7f4d6906591e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2224, 200)\n",
            "(557, 200)\n"
          ]
        }
      ],
      "source": [
        "def dataset2featureMatrix(dataset, embeddingModel):\n",
        "\n",
        "    X_embedding = []\n",
        "    for doc in dataset:\n",
        "        vec = getDocvector(embeddingModel, doc)\n",
        "        X_embedding.append(vec)\n",
        "\n",
        "    X_embedding = np.array(X_embedding)\n",
        "    return X_embedding\n",
        "\n",
        "\n",
        "X_train_embedding = dataset2featureMatrix(dataset2_train, embeddingModel)\n",
        "X_test_embedding = dataset2featureMatrix(dataset2_test, embeddingModel)\n",
        "\n",
        "print(X_train_embedding.shape)\n",
        "print(X_test_embedding.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6MKEqRiyYuC"
      },
      "source": [
        "_______\n",
        "# Treinando um modelo de classificação por regressão logística\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "OmAgnxBJyYuD"
      },
      "outputs": [],
      "source": [
        "def classifica(X_train, X_test,\n",
        "                Y_train, Y_test,\n",
        "                formato = \"TF\"):\n",
        "\n",
        "  model = skl.linear_model.LogisticRegression(\n",
        "      C = 0.5, max_iter=500,\n",
        "      random_state = 0\n",
        "  )\n",
        "\n",
        "  # normaliza os dados\n",
        "  if formato==\"embedding\":\n",
        "    scaler = skl.preprocessing.StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "  elif formato==\"TF\":\n",
        "    scaler = skl.preprocessing.Normalizer(norm='l2')\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "  # treinando o método de classificação\n",
        "  model.fit( X_train, Y_train )\n",
        "\n",
        "  # classifica os dados de teste\n",
        "  Y_pred = model.predict(X_test)\n",
        "\n",
        "  print(\"\\nAcurácia: \", skl.metrics.accuracy_score(Y_test, Y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "MoZ9uwmxOxaV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2c318fa-11fc-4178-ec62-1a87006f5f93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classificação Binário\n",
            "\n",
            "Acurácia:  0.9569120287253142\n",
            "\n",
            "Classificação TF\n",
            "\n",
            "Acurácia:  0.9210053859964094\n",
            "\n",
            "Classificação TF-IDF\n",
            "\n",
            "Acurácia:  0.9425493716337523\n",
            "\n",
            "Classificação word embbeddings\n",
            "\n",
            "Acurácia:  0.9515260323159784\n"
          ]
        }
      ],
      "source": [
        "print(\"Classificação Binário\")\n",
        "classifica(X_train_bin, X_test_bin, Y_train, Y_test,\n",
        "            formato = \"binario\")\n",
        "\n",
        "print(\"\\nClassificação TF\")\n",
        "classifica(X_train_tf, X_test_tf, Y_train, Y_test,\n",
        "            formato = \"TF\")\n",
        "\n",
        "print(\"\\nClassificação TF-IDF\")\n",
        "classifica(X_train_tfidf, X_test_tfidf, Y_train, Y_test,\n",
        "            formato = \"TF\")\n",
        "\n",
        "print(\"\\nClassificação word embbeddings\")\n",
        "classifica(X_train_embedding, X_test_embedding, Y_train, Y_test,\n",
        "            formato = \"embedding\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_WSRbAT2UH5"
      },
      "source": [
        "# Escolha dos Hiperparâmetros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLLjDUzV2ZWQ"
      },
      "source": [
        "Grid Search para definir o custo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "0N0sgwyi1SHi"
      },
      "outputs": [],
      "source": [
        "def buscar_melhores_hiperparametros(X_train, Y_train):\n",
        "    # Definindo o modelo com suporte a multiclasse\n",
        "    model = LogisticRegression(\n",
        "        max_iter=500,\n",
        "        random_state=0,\n",
        "        multi_class='multinomial',  # Alterado para 'multinomial' para suporte multiclasse\n",
        "        solver='lbfgs'              # Solver compatível com 'multinomial'\n",
        "    )\n",
        "\n",
        "    # Definindo os valores dos hiperparâmetros para testar\n",
        "    param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
        "\n",
        "    # Definindo o scorer AUC para multiclasse\n",
        "    auc_scorer = make_scorer(roc_auc_score, needs_proba=True, multi_class='ovr')\n",
        "\n",
        "    # Inicializando a busca em grade com validação cruzada\n",
        "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring=auc_scorer)\n",
        "\n",
        "    # Treinando o modelo com a busca em grade\n",
        "    grid_search.fit(X_train, Y_train)\n",
        "\n",
        "    # Retorna o melhor modelo e o melhor valor de 'C' encontrado\n",
        "    return grid_search.best_estimator_, grid_search.best_params_['C']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "9LzYYHS11LLa"
      },
      "outputs": [],
      "source": [
        "def classificar_com_melhores_hiperparametros(X_train, X_test, Y_train, Y_test, formato):\n",
        "    # Normaliza os dados\n",
        "    if formato == \"embedding\":\n",
        "        scaler = skl_preprocessing.StandardScaler()\n",
        "        X_train = scaler.fit_transform(X_train)\n",
        "        X_test = scaler.transform(X_test)\n",
        "    elif formato == \"TF\":\n",
        "        scaler = skl_preprocessing.Normalizer(norm='l2')\n",
        "        X_train = scaler.fit_transform(X_train)\n",
        "        X_test = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "    # Encontrando os melhores hiperparâmetros\n",
        "    best_model, melhores_parametros = buscar_melhores_hiperparametros(X_train, Y_train)\n",
        "    print(f\"Melhores Hiperparâmetros: {melhores_parametros}\")\n",
        "\n",
        "    # Treinando o método de classificação com os melhores hiperparâmetros\n",
        "    best_model.fit(X_train, Y_train)\n",
        "\n",
        "    # Classifica os dados de teste\n",
        "    Y_pred = best_model.predict(X_test)\n",
        "\n",
        "    # Relatório de classificação\n",
        "    print(\"\\nRelatório de Classificação:\\n\", classification_report(Y_test, Y_pred))\n",
        "\n",
        "    # Imprime a acurácia e AUC\n",
        "    accuracy = skl_metrics.accuracy_score(Y_test, Y_pred)\n",
        "    print(f\"Acurácia: {accuracy:.4f}\")\n",
        "\n",
        "    y_proba = best_model.predict_proba(X_test)\n",
        "    auc = roc_auc_score(Y_test, y_proba, multi_class='ovo')\n",
        "    print(f\"AUC: {auc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Testando TF"
      ],
      "metadata": {
        "id": "ZKpItrz4in7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_model_tf_grid = classificar_com_melhores_hiperparametros(X_train_tf, X_test_tf, Y_train, Y_test, 'TF')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mL83D4Q3bEb8",
        "outputId": "bc037a69-01ba-479e-8342-35a0e4ef3151"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Melhores Hiperparâmetros: 100\n",
            "\n",
            "Relatório de Classificação:\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "CorpIndustrial       0.50      0.29      0.36         7\n",
            "      Economia       0.90      0.95      0.93       118\n",
            "     GovSocial       0.85      0.61      0.71        18\n",
            "      Mercados       0.99      0.99      0.99       414\n",
            "\n",
            "      accuracy                           0.96       557\n",
            "     macro avg       0.81      0.71      0.75       557\n",
            "  weighted avg       0.96      0.96      0.96       557\n",
            "\n",
            "Acurácia: 0.9605\n",
            "AUC: 0.9493\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Testando TF-IDF"
      ],
      "metadata": {
        "id": "cSkDm7DCisJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_model_tfidf_grid = classificar_com_melhores_hiperparametros(X_train_tfidf, X_test_tfidf, Y_train, Y_test, 'TF')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEYYt8FUi2yn",
        "outputId": "81260f9e-50aa-4f1a-eece-5d5c789250f7"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Melhores Hiperparâmetros: 100\n",
            "\n",
            "Relatório de Classificação:\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "CorpIndustrial       0.67      0.29      0.40         7\n",
            "      Economia       0.90      0.95      0.92       118\n",
            "     GovSocial       0.86      0.67      0.75        18\n",
            "      Mercados       0.99      0.99      0.99       414\n",
            "\n",
            "      accuracy                           0.96       557\n",
            "     macro avg       0.85      0.72      0.76       557\n",
            "  weighted avg       0.96      0.96      0.96       557\n",
            "\n",
            "Acurácia: 0.9605\n",
            "AUC: 0.9561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Testando Binário"
      ],
      "metadata": {
        "id": "utj-J_waiv_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_model_tfbin_grid = classificar_com_melhores_hiperparametros(X_train_bin, X_test_bin, Y_train, Y_test, 'binario')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_s_nZN8Bi6Dt",
        "outputId": "c857c4ed-82df-4dbf-f80f-0e318f14205a"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Melhores Hiperparâmetros: 1\n",
            "\n",
            "Relatório de Classificação:\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "CorpIndustrial       0.50      0.29      0.36         7\n",
            "      Economia       0.90      0.96      0.93       118\n",
            "     GovSocial       0.73      0.44      0.55        18\n",
            "      Mercados       0.98      0.99      0.99       414\n",
            "\n",
            "      accuracy                           0.96       557\n",
            "     macro avg       0.78      0.67      0.71       557\n",
            "  weighted avg       0.95      0.96      0.95       557\n",
            "\n",
            "Acurácia: 0.9551\n",
            "AUC: 0.9525\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Testando Word Embeddings"
      ],
      "metadata": {
        "id": "ggpjLfHpizZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_model_embedding_grid = classificar_com_melhores_hiperparametros(X_train_embedding, X_test_embedding, Y_train, Y_test, 'embedding')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QXxQjSqi_1G",
        "outputId": "d08c5a99-ea93-471e-fa4b-bbe424ec8202"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Melhores Hiperparâmetros: 0.1\n",
            "\n",
            "Relatório de Classificação:\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "CorpIndustrial       0.60      0.43      0.50         7\n",
            "      Economia       0.88      0.97      0.92       118\n",
            "     GovSocial       1.00      0.50      0.67        18\n",
            "      Mercados       0.99      0.99      0.99       414\n",
            "\n",
            "      accuracy                           0.96       557\n",
            "     macro avg       0.87      0.72      0.77       557\n",
            "  weighted avg       0.96      0.96      0.96       557\n",
            "\n",
            "Acurácia: 0.9587\n",
            "AUC: 0.9064\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Previsão do conjunto de teste"
      ],
      "metadata": {
        "id": "PKKgHS6_lYgL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para preparar os dados de teste\n",
        "def prepare_test_data(df_test):\n",
        "    # Aplicar a função de extração de textos do XML no DataFrame de teste\n",
        "    df_test = apply_extraction(df_test)\n",
        "\n",
        "    # Re-organizar as colunas do DataFrame\n",
        "    df_test = df_test[['ID', 'TITULO', 'TEXTO']]\n",
        "\n",
        "    # Aplicar a função de pré-processamento no texto das amostras de teste\n",
        "    df_test['TEXTO'] = df_test['TEXTO'].apply(preprocessing_portuguese)\n",
        "\n",
        "    return df_test"
      ],
      "metadata": {
        "id": "Mx_iBaazlh8S"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classificar_e_treinar_com_melhores_hiperparametros(X_train, X_val, y_train, y_val):\n",
        "    # Encontrando os melhores hiperparâmetros\n",
        "    best_model, melhores_parametros = buscar_melhores_hiperparametros(X_train, y_train)\n",
        "    print(f\"Melhores Hiperparâmetros: {melhores_parametros}\")\n",
        "\n",
        "    scaler = skl_preprocessing.Normalizer(norm='l2')\n",
        "    X_val = scaler.fit_transform(X_val)\n",
        "\n",
        "\n",
        "    # Fazendo previsões no conjunto de validação\n",
        "    y_pred = best_model.predict(X_val)\n",
        "\n",
        "    # Avaliando a acurácia\n",
        "    accuracy = skl_metrics.accuracy_score(y_val, y_pred)\n",
        "    print(f\"Acurácia: {accuracy:.4f}\")\n",
        "\n",
        "    # Relatório de classificação\n",
        "    print(\"\\nRelatório de Classificação:\\n\", skl_metrics.classification_report(y_val, y_pred,zero_division=0))\n",
        "\n",
        "    # Matriz de confusão\n",
        "    print(\"\\nMatriz de Confusão:\\n\", skl_metrics.confusion_matrix(y_val, y_pred))\n",
        "\n",
        "    # Calculando a AUC\n",
        "    y_proba = best_model.predict_proba(X_val)\n",
        "    auc = roc_auc_score(y_val, y_proba, multi_class='ovo')\n",
        "    print(f\"AUC: {auc:.4f}\")\n",
        "\n",
        "    return best_model\n"
      ],
      "metadata": {
        "id": "jafI0xjxligk"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para obter as probabilidades e gerar o arquivo de submissão\n",
        "def generate_submission_file(classifier, X_test, df_test, filename='submission.csv'):\n",
        "    # Obter as probabilidades das classes\n",
        "    y_proba = classifier.predict_proba(X_test)\n",
        "\n",
        "    # Criar um DataFrame com as probabilidades\n",
        "    submission_df = pd.DataFrame(y_proba, columns=le.classes_)\n",
        "\n",
        "    # Adicionar a coluna ID\n",
        "    submission_df.insert(0, 'ID', df_test['ID'])\n",
        "\n",
        "    # Renomear as colunas para o formato exigido\n",
        "    submission_df.columns = ['ID', 'CorpIndustrial', 'Economia', 'GovSocial', 'Mercados']\n",
        "\n",
        "    # Salvar o DataFrame como um arquivo CSV\n",
        "    submission_df.to_csv(filename, index=False, float_format='%.5f')\n",
        "    print(f\"Arquivo de submissão salvo como {filename}\")"
      ],
      "metadata": {
        "id": "QWXBe7n0lv9V"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregar e preparar os dados de teste\n",
        "df_test = prepare_test_data(df_teste)"
      ],
      "metadata": {
        "id": "PZwyGL2wmCOu"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Como o TF-IDF teve a maior AUC, então será aplicado apenas esse metodo\n",
        "X_test_tf = vectorizer.transform(df_test['TEXTO'])\n",
        "X_test_tfidf = tfidf_model.transform(X_test_tf)"
      ],
      "metadata": {
        "id": "LRinGfBkmEgm"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Divisão dos dados de treino em treino e validação\n",
        "X_train_part, X_val_part, y_train_part, y_val_part = train_test_split(X_train_tfidf, Y_train, test_size=0.2, random_state=42, stratify=Y_train)\n",
        "\n",
        "# Treinar e avaliar o modelo com busca aleatória para TF-IDF\n",
        "print(\"\\n\\nTreinando com o formato TF-IDF usando busca por grid\")\n",
        "print(X_train_part.shape, X_val_part.shape, y_train_part.shape)\n",
        "best_model_tfidf = classificar_e_treinar_com_melhores_hiperparametros(X_train_part, X_val_part, y_train_part, y_val_part)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWDKxJ02mKp1",
        "outputId": "c03a7389-9406-41f4-aae5-0bc7a01f92fc"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Treinando com o formato TF-IDF usando busca por grid\n",
            "(1779, 9506) (445, 9506) (1779,)\n",
            "Melhores Hiperparâmetros: 100\n",
            "Acurácia: 0.9843\n",
            "\n",
            "Relatório de Classificação:\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "CorpIndustrial       1.00      0.50      0.67         6\n",
            "      Economia       0.97      0.98      0.97        94\n",
            "     GovSocial       0.87      0.93      0.90        14\n",
            "      Mercados       0.99      1.00      1.00       331\n",
            "\n",
            "      accuracy                           0.98       445\n",
            "     macro avg       0.96      0.85      0.88       445\n",
            "  weighted avg       0.98      0.98      0.98       445\n",
            "\n",
            "\n",
            "Matriz de Confusão:\n",
            " [[  3   1   1   1]\n",
            " [  0  92   1   1]\n",
            " [  0   1  13   0]\n",
            " [  0   1   0 330]]\n",
            "AUC: 0.9882\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gerar o arquivo de submissão usando o modelo treinado\n",
        "generate_submission_file(best_model_tfidf, X_test_tfidf, df_test, filename='submission_files/regressao/submission_tfidf.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ESfKG3fIwoI",
        "outputId": "f562ff37-bc36-4a56-be10-2022b52fa0b9"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arquivo de submissão salvo como submission_files/regressao/submission_tfidf.csv\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}