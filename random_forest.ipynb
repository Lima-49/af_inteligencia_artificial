{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vitor\\OneDrive\\Documentos\\Faculdade\\inteligencia artificial\\af_inteligencia_artificial\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from sklearn import feature_extraction\n",
    "import sklearn as skl\n",
    "from nltk.stem import RSLPStemmer\n",
    "import nltk\n",
    "from zipfile import ZipFile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import os\n",
    "import re\n",
    "import gensim\n",
    "import scipy.sparse\n",
    "from gensim.models import Word2Vec\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.regularizers import L2\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForPreTraining\n",
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminho dos arquivos extraidos do kaggle\n",
    "path_dataset = r'files\\classificao-de-notcias.zip'\n",
    "path_db = r'db'\n",
    "\n",
    "# Caminho dos arquivos que serão utilizados para a atividade\n",
    "path_train = r'db\\arquivos_competicao\\arquivos_competicao\\train.csv'\n",
    "path_test = r'db\\arquivos_competicao\\arquivos_competicao\\test.csv'\n",
    "path_news = r'db\\arquivos_competicao\\arquivos_competicao\\news'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo já descompactado\n"
     ]
    }
   ],
   "source": [
    "def unzip(path, pathFolder):\n",
    "\n",
    "    # descompacta a base de dados de notícias\n",
    "    z = ZipFile(path, 'r')\n",
    "\n",
    "    if os.path.isdir(pathFolder):\n",
    "        z.extractall(pathFolder)\n",
    "        z.close()\n",
    "    else:\n",
    "        os.mkdir(pathFolder)\n",
    "        z.extractall(pathFolder)\n",
    "        z.close()\n",
    "\n",
    "    print(\"Arquivo descompactado com sucesso!\")\n",
    "    \n",
    "# Antes de descompactar os arquivos valida se ja foram descompactados antes\n",
    "if not os.path.isdir(path_news):\n",
    "    unzip(path_dataset, path_db)\n",
    "else:\n",
    "    print(\"Arquivo já descompactado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Carregando os arquivos de teste e treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colunas do arquivo de treino:  Index(['ID', 'Class'], dtype='object')\n",
      "Quantidade de linhas:  2781\n"
     ]
    }
   ],
   "source": [
    "#Carregando os arquivos de treino\n",
    "df_train = pd.read_csv(path_train)\n",
    "print(\"Colunas do arquivo de treino: \", df_train.columns)\n",
    "print(\"Quantidade de linhas: \", df_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colunas do arquivo de treino:  Index(['ID'], dtype='object')\n",
      "Quantidade de linhas:  1193\n"
     ]
    }
   ],
   "source": [
    "# Carregando os arquivos de teste\n",
    "df_teste = pd.read_csv(path_test)\n",
    "print(\"Colunas do arquivo de treino: \", df_teste.columns)\n",
    "print(\"Quantidade de linhas: \", df_teste.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pré-Processamento dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_xml_text(path_xml):\n",
    "    \n",
    "    \"\"\"\n",
    "    A função `extract_xml_text` é designada para extrair os textos dos arquivos XML\n",
    "    especificados pelo `path_xml`.\n",
    "    Utilizando a biblioteca ElementTree para leitura do arquivo XML \n",
    "    \"\"\"\n",
    "    \n",
    "    # Instancia um objeto como uma árvore de análise\n",
    "    tree = ET.parse(path_xml)\n",
    "    \n",
    "    # Obtem o elemento raiz da árvore de ánalise\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    # Encontra o elemento headline (titulo) dentro da árvore de analise\n",
    "    headline = root.find('headline').text if root.find('headline') is not None else ''\n",
    "    \n",
    "    # Entroa todos os elementos <p> que na estrutura dos xml's contem o texto\n",
    "    paragraphs = root.findall('.//p')\n",
    "    \n",
    "    # Junta em uma unica string, separando por espaços\n",
    "    text = ' '.join([p.text for p in paragraphs if p.text is not None])\n",
    "\n",
    "    return headline, text\n",
    "\n",
    "def apply_extraction(df_applyed):\n",
    "\n",
    "    \"\"\"\n",
    "    Essa função é responsável em aplicar as novas colunas \n",
    "    no df_applyed passado como parametro.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Loop pelas linhas do df\n",
    "    for idx in df_applyed.index:\n",
    "        \n",
    "        # atribui o valor da coluna id na variavel file\n",
    "        file = df_applyed.at[idx, 'ID']\n",
    "        \n",
    "        # Concatena o nome do arquivo com o caminho dele\n",
    "        path_xml = f\"{path_news}\\{file}\"\n",
    "        \n",
    "        # Extrai o texto e titulo desse arquivo\n",
    "        titulo, texto = extract_xml_text(path_xml)\n",
    "        \n",
    "        #Atribui esses o texto e titulos em novas colunas\n",
    "        df_applyed.at[idx, 'TITULO'] = titulo\n",
    "        df_applyed.at[idx, 'TEXTO'] = texto\n",
    "    \n",
    "    return df_applyed\n",
    "\n",
    "def print_porcentagem(target):\n",
    "    # Calcula a contagem de cada classe\n",
    "    class_counts = target['Class'].value_counts()\n",
    "    \n",
    "    # Calcula a porcentagem de cada classe\n",
    "    class_percentages = class_counts / len(target) * 100\n",
    "    \n",
    "    # Imprime a porcentagem de cada classe\n",
    "    for cl, pct in class_percentages.items():\n",
    "        print(f\"Porcentagem da classe {cl}: {round(pct, 2)}%\")\n",
    "        \n",
    "def preprocessing_portuguese(text, stemming = False, stopwords = False):\n",
    "    \"\"\"\n",
    "    Funcao usada para tratar textos escritos na lingua portuguesa\n",
    "\n",
    "    Parametros:\n",
    "        text: variavel do tipo string que contem o texto que devera ser tratado\n",
    "\n",
    "        stemming: variavel do tipo boolean que indica se a estemizacao deve ser aplicada ou nao\n",
    "\n",
    "        stopwords: variavel do tipo boolean que indica se as stopwords devem ser removidas ou nao\n",
    "    \"\"\"\n",
    "\n",
    "    # Lower case\n",
    "    text = text.lower()\n",
    "\n",
    "    # remove os acentos das palavras\n",
    "    nfkd_form = unicodedata.normalize('NFKD', text)\n",
    "    text = u\"\".join([c for c in nfkd_form if not unicodedata.combining(c)])\n",
    "\n",
    "    # remove tags HTML\n",
    "    regex = re.compile('<[^<>]+>')\n",
    "    text = re.sub(regex, \" \", text)\n",
    "\n",
    "    # normaliza as URLs\n",
    "    regex = re.compile('(http|https)://[^\\s]*')\n",
    "    text = re.sub(regex, \"<URL>\", text)\n",
    "\n",
    "    # normaliza emails\n",
    "    regex = re.compile('[^\\s]+@[^\\s]+')\n",
    "    text = re.sub(regex, \"<EMAIL>\", text)\n",
    "\n",
    "    # converte todos os caracteres não-alfanuméricos em espaço\n",
    "    regex = re.compile('[^A-Za-z0-9]+')\n",
    "    text = re.sub(regex, \" \", text)\n",
    "\n",
    "    # normaliza os numeros\n",
    "    regex = re.compile('[0-9]+.[0-9]+')\n",
    "    text = re.sub(regex, \"NUMERO\", text)\n",
    "\n",
    "    # normaliza os numeros\n",
    "    regex = re.compile('[0-9]+,[0-9]+')\n",
    "    text = re.sub(regex, \"NUMERO\", text)\n",
    "\n",
    "    # normaliza os numeros\n",
    "    regex = re.compile('[0-9]+')\n",
    "    text = re.sub(regex, \"NUMERO\", text)\n",
    "\n",
    "\n",
    "    # substitui varios espaçamentos seguidos em um só\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    # separa o texto em palavras\n",
    "    words = text.split()\n",
    "\n",
    "    # trunca o texto para apenas 200 termos\n",
    "    words = words[0:200]\n",
    "\n",
    "    # remove stopwords\n",
    "    if stopwords:\n",
    "        words = text.split() # separa o texto em palavras\n",
    "        words = [w for w in words if not w in nltk.corpus.stopwords.words('portuguese')]\n",
    "        text = \" \".join( words )\n",
    "\n",
    "    # aplica estemização\n",
    "    if stemming:\n",
    "        stemmer_method = RSLPStemmer()\n",
    "        words = [ stemmer_method.stem(w) for w in words ]\n",
    "        text = \" \".join( words )\n",
    "\n",
    "    # remove palavras compostas por apenas um caracter\n",
    "    words = text.split() # separa o texto em palavras\n",
    "    words = [ w for w in words if len(w)>1 ]\n",
    "    text = \" \".join( words )\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porcentagem da classe Mercados: 74.33%\n",
      "Porcentagem da classe Economia: 21.18%\n",
      "Porcentagem da classe GovSocial: 3.24%\n",
      "Porcentagem da classe CorpIndustrial: 1.26%\n"
     ]
    }
   ],
   "source": [
    "#Aplicando a função de exracao dos textos do xml no DataFrame de treino\n",
    "df_train = apply_extraction(df_train)\n",
    "\n",
    "#Re-organizando as colunas do df\n",
    "df_train = df_train[['ID', 'TITULO', 'TEXTO', 'Class']]\n",
    "\n",
    "#Printando as classes e as suas respectativas porcentagens\n",
    "print_porcentagem(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratando os textos da base de dados\n",
    "- Aplicada a função de estemização para a linguagem dos textos (português)\n",
    "- Removendo os ascentos das palavras\n",
    "- Criando um limite de 200 temrmos por palavras, para evitar que a predição do classificador seja influenciada pelo tamanho da noticia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vitor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package rslp to\n",
      "[nltk_data]     C:\\Users\\vitor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the stopwords corpus\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Download the RSLPStemmer\n",
    "nltk.download('rslp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar a função ao DataFrame de treino\n",
    "df_train['TEXTO'] = df_train['TEXTO'].apply(preprocessing_portuguese)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformando a coluna target para valores numericos\n",
    "Já que o problema a ser lidado, possui diversas classes, e o modelo a ser testado é o modelo Random Forest que não são sensiveis à ordem da categoria, para a transoformação será utilizado o Label Encoding da biblioteca sklearn, que converte cada categoria em um numero inteiro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformação do target\n",
    "le = LabelEncoder()\n",
    "df_train['Class'] = le.fit_transform(df_train['Class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separação dos dados entre dados de teste e treino\n",
    "\n",
    "Foi utilizada a função train_test_split do scikit-learn para dividir o conjunto de dados para ter uma ideia rápida do desempenho do modelo\n",
    "Os paramentros utilizados nessa separação foram:\n",
    "- **features**: o conjunto de textos extraido do arquivo XML\n",
    "- **target**: as classes já convertidas para numerico\n",
    "- **test_size**=0.2: divisão de 20% dos dados para teste\n",
    "- **random_state**=42: obter a mesma divisão toda vez que executar o código\n",
    "- **stritify**=target: parametro utilizado para garantir que as proporções das classes sejam mantidas nos conjuntos de treino e teste já que as clasesses do nosso problema estão desbalanceadas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divisão dos dados em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train['TEXTO'],\n",
    "                                                    df_train['Class'],\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=df_train['Class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Técnicas de representação vetorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF (Term-Frequency)\n",
    "Utilizando a função **CountVectorizer** da biblioteca **scikit-learn** para criar uma representação vetorial dos textos utilizando a frequencia de cada palavra\n",
    "\n",
    "Os parametros passados para essa estancia foram:\n",
    "- **analyzer=word**: Define o nível de análise de tokens para palavras (word0)\n",
    "- **dtype=np.int32**: Tipo de dados da matriz de contagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 primeiras palavras do vocabulário obtidas a partir dos dados de treinamento:\n",
      "\n",
      "['aa' 'aaa' 'aanumero' 'aas' 'abaixo' 'abaixos' 'abaixou' 'abalaram'\n",
      " 'abanadas' 'abanar' 'abandona' 'abandonado' 'abandonaram' 'abastecimento'\n",
      " 'abatidas' 'abatimentos' 'abel' 'aberta' 'abertamente' 'abertas']\n",
      "\n",
      "Dimensão dos dados vetorizados de treino:  (2224, 9509)\n",
      "\n",
      "Dimensão dos dados vetorizados de teste:  (557, 9509)\n"
     ]
    }
   ],
   "source": [
    "# inicializa o modelo usado para gerar a representação TF (term frequency)\n",
    "vectorizer = CountVectorizer(analyzer=\"word\",dtype=np.int32)\n",
    "\n",
    "# treina o modelo TF com os dados de treinamento e converte os dados de treinamento para uma array \n",
    "# que contém a frequência dos termos em cada documento (TF - term frequency)\n",
    "X_train_tf = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# converte os dados de teste\n",
    "X_test_tf = vectorizer.transform(X_test)\n",
    "\n",
    "\n",
    "print('20 primeiras palavras do vocabulário obtidas a partir dos dados de treinamento:\\n')\n",
    "print(vectorizer.get_feature_names_out()[0:20])\n",
    "\n",
    "print('\\nDimensão dos dados vetorizados de treino: ', X_train_tf.shape)\n",
    "print('\\nDimensão dos dados vetorizados de teste: ', X_test_tf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF (Term Frequency-Inverse Document Frequency).\n",
    "Utilizando a função **TfidfTransformer** para para converter a representação vetorial TF para TF-IDF\n",
    "Os parametros utilizados nessa função foram:\n",
    "- **norm=l2**: normalizando cada vetor TF-IDF para que a soma dos quadrados dos elementos seja igual a 1\n",
    "- **smooth_idf**: adiciona 1 ao denominador da formula de IDF para eviter divisoes por zero\n",
    "- **sublinear_tf**: aplica a sublinearização do TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2224, 9509)\n",
      "(557, 9509)\n"
     ]
    }
   ],
   "source": [
    "#Inicializa o modelo usado para gerar a representação TF-IDF\n",
    "tfidf_model = skl.feature_extraction.text.TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
    "\n",
    "# Treina o modelo com os vetores de treinamento\n",
    "X_train_tfidf = tfidf_model.fit_transform(X_train_tf)\n",
    "\n",
    "# treina o modelo com os dados de teste\n",
    "X_test_tfidf = tfidf_model.transform(X_test_tf)\n",
    "\n",
    "print(X_train_tfidf.shape)\n",
    "print(X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binário\n",
    "utilizando a técnica de vetorização binária onde cada termo é representado como um valor binário. Ou seja, se o termo aparece no documento o valor é 1, caso contrário o valor é 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2224, 9509)\n",
      "(557, 9509)\n"
     ]
    }
   ],
   "source": [
    "X_train_bin = X_train_tf.copy()\n",
    "X_test_bin = X_test_tf.copy()\n",
    "\n",
    "#convert os dados de treino para representação binária\n",
    "X_train_bin[X_train_bin!=0]=1\n",
    "\n",
    "#convert os dados de teste para representação binária\n",
    "X_test_bin[X_test_bin!=0]=1\n",
    "\n",
    "print(X_train_bin.shape)\n",
    "print(X_test_bin.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings (treinada com a própria base)\n",
    "Os word embeddings são uma técnica avançada de representação vetorial de palavras que capturam o significado semântico e as relações entre palavras em um espaço vetorial de alta dimensão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 primeiras palavras da primeira amostra de treino\n",
      "['lisboa', 'NUMERO', 'mar', 'reuter', 'banco', 'de', 'portugal', 'deve', 'manter', 'inalteradas', 'todas', 'as', 'suas', 'taxas', 'directoras', 'no', 'inicio', 'do', 'proximo', 'periodo', 'de', 'reservas', 'consideram', 'sete', 'de', 'um', 'conjunto', 'de', 'NUMERO', 'economistas']\n",
      "\n",
      "20 primeiras palavras da primeira amostra de teste\n",
      "['escudo', 'fechou', 'em', 'alta', 'contra', 'marco', 'numa', 'sessao', 'volatil', 'marcada', 'pela', 'forte', 'probabilidade', 'da', 'reentrada', 'da', 'lira', 'no', 'mtc', 'do', 'sme', 'dealers', 'disseram', 'que', 'escudo', 'devera', 'manter', 'se', 'colado', 'aos']\n"
     ]
    }
   ],
   "source": [
    "# Tokenização dos textos de treinamento\n",
    "train_documents = [text.split() for text in X_train]\n",
    "\n",
    "# Tokenização dos textos de teste\n",
    "test_documents = [text.split() for text in X_test]\n",
    "\n",
    "print(\"20 primeiras palavras da primeira amostra de treino\")\n",
    "print(train_documents[0][0:30])\n",
    "\n",
    "print(\"\\n20 primeiras palavras da primeira amostra de teste\")\n",
    "print(test_documents[0][0:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função **Word2Vec** foi utilizada para treinar o modelo de word embeddings utilizando os seguintes paramentros:\n",
    "- **sentences**: a lista das listas de palavras tokenizadas anteriormente\n",
    "- **vector_size=200**: define que cada vetor tera 200 dimensões\n",
    "- **window=3**: considera 3 palavras à esquerda e 3 palavras à direita da palavra-alvo\n",
    "- **min_count=1**: inclui todas as palavras que aparecem pelo menos uma vez\n",
    "- **workers=4**: usa 4 threads para o treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do vocabulario do modelo:  9510\n"
     ]
    }
   ],
   "source": [
    "w2v_model = Word2Vec(train_documents,\n",
    "                     vector_size=200,\n",
    "                     window=3,\n",
    "                     min_count=1,\n",
    "                     workers=4)\n",
    "\n",
    "print(\"Tamanho do vocabulario do modelo: \", len(w2v_model.wv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDocvector(model, doc):\n",
    "    # Inicializa uma lista para armazenar os vetores das palavras\n",
    "    wordList = []\n",
    "    \n",
    "    # Itera sobre cada palavra no documento\n",
    "    for word in doc:\n",
    "        try:\n",
    "            # Tenta obter o vetor da palavra do modelo Word2Vec\n",
    "            vec = model.wv[word]\n",
    "            # Adiciona o vetor da palavra à lista\n",
    "            wordList.append(vec)\n",
    "        except:\n",
    "            # Se a palavra não estiver no vocabulário do modelo, ignora\n",
    "            pass\n",
    "    \n",
    "    # Se a lista de vetores de palavras não estiver vazia\n",
    "    if len(wordList) > 0:\n",
    "        # Calcula a média dos vetores das palavras para representar o documento\n",
    "        vetorMedio = np.mean(wordList, axis=0)\n",
    "    else:\n",
    "        # Se nenhuma palavra do documento estiver no vocabulário do modelo,\n",
    "        # retorna um vetor de zeros com o mesmo tamanho dos vetores do modelo\n",
    "        vetorMedio = np.zeros(model.wv.vector_size)\n",
    "    \n",
    "    # Retorna o vetor médio do documento\n",
    "    return vetorMedio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset2featureMatrix(dataset, embeddingModel):\n",
    "    # Inicializa uma lista para armazenar os embeddings dos documentos\n",
    "    X_embedding = []\n",
    "    \n",
    "    # Itera sobre cada documento no dataset\n",
    "    for doc in dataset:\n",
    "        # Obtém o vetor médio do documento usando a função getDocvector\n",
    "        vec = getDocvector(embeddingModel, doc)\n",
    "        # Adiciona o vetor do documento à lista de embeddings\n",
    "        X_embedding.append(vec)\n",
    "    \n",
    "    # Converte a lista de embeddings em um array NumPy\n",
    "    X_embedding = np.array(X_embedding)\n",
    "    \n",
    "    # Retorna a matriz de features\n",
    "    return X_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2224, 200)\n",
      "(557, 200)\n"
     ]
    }
   ],
   "source": [
    "# Representação Vetorial com Word2Vec treinado na própria base\n",
    "X_train_embedding = dataset2featureMatrix(train_documents, w2v_model)\n",
    "X_test_embedding = dataset2featureMatrix(test_documents, w2v_model)\n",
    "\n",
    "print(X_train_embedding.shape)\n",
    "print(X_test_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings (pré-treinadas)\n",
    "Para esse caso será utilizado um modelo pré-treinado para português postado no Hugging Face, chamado [BERTimbau](https://huggingface.co/neuralmind/bert-base-portuguese-cased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
    "tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(model_output, attention_masks):\n",
    "    # Extrai os embeddings dos tokens da saída do modelo\n",
    "    tokenEmbeddings = model_output[0]\n",
    "\n",
    "    # Expande a máscara de atenção para ter as mesmas dimensões que os embeddings dos tokens\n",
    "    inputExpandido = attention_masks.unsqueeze(-1).expand(tokenEmbeddings.size())\n",
    "\n",
    "    # Converte a máscara de atenção expandida para float\n",
    "    inputExpandido = inputExpandido.float()\n",
    "\n",
    "    # Calcula a média ponderada dos embeddings dos tokens\n",
    "    # Multiplica os embeddings dos tokens pela máscara de atenção expandida\n",
    "    # Depois soma os embeddings ao longo da dimensão dos tokens (dimensão 1)\n",
    "    # Divide pela soma das máscaras de atenção para normalizar\n",
    "    saida = (torch.sum(tokenEmbeddings * inputExpandido, 1) /\n",
    "             torch.clamp(inputExpandido.sum(1), min=0.0000001))\n",
    "\n",
    "    # Retorna os embeddings da sentença\n",
    "    return saida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_docVec(model, tokenizer, doc):\n",
    "    # Tokeniza o documento, adicionando padding e truncamento, e converte para tensores PyTorch\n",
    "    encoded = tokenizer(doc,\n",
    "                        padding=True,\n",
    "                        truncation=True,\n",
    "                        max_length=200,\n",
    "                        return_tensors=\"pt\")\n",
    "\n",
    "    # Desabilita a computação de gradiente para economizar memória e acelerar a inferência\n",
    "    with torch.no_grad():\n",
    "        # Gera a saída do modelo BERT para o documento tokenizado\n",
    "        model_output = model(**encoded)\n",
    "\n",
    "    # Calcula o embedding da sentença usando a média ponderada dos embeddings dos tokens\n",
    "    sentenceEmbedding = mean_pooling(model_output, encoded['attention_mask'])\n",
    "\n",
    "    # Converte o embedding da sentença para um array NumPy\n",
    "    sentenceEmbedding = sentenceEmbedding.squeeze().numpy()\n",
    "\n",
    "    # Retorna o embedding da sentença\n",
    "    return sentenceEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset2featureMatrixBert(dataset, model, tokenizer):\n",
    "    # Inicializa uma lista para armazenar os embeddings dos documentos\n",
    "    X_embedding = []\n",
    "\n",
    "    # Itera sobre cada documento no dataset\n",
    "    for doc in dataset:\n",
    "        # Obtém o vetor médio do documento usando a função get_docVec\n",
    "        vec = get_docVec(model, tokenizer, doc)\n",
    "        # Adiciona o vetor do documento à lista de embeddings\n",
    "        X_embedding.append(vec)\n",
    "\n",
    "    # Converte a lista de embeddings em um array NumPy\n",
    "    return np.array(X_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensões dos embeddings de treinamento: (2224, 768)\n",
      "Dimensões dos embeddings de teste: (557, 768)\n"
     ]
    }
   ],
   "source": [
    "# Geração das representações vetoriais para os conjuntos de treinamento e teste\n",
    "X_train_embedding_bert = dataset2featureMatrixBert(X_train, model, tokenizer)\n",
    "X_test_embedding_bert = dataset2featureMatrixBert(X_test, model, tokenizer)\n",
    "\n",
    "print(\"Dimensões dos embeddings de treinamento:\", X_train_embedding_bert.shape)\n",
    "print(\"Dimensões dos embeddings de teste:\", X_test_embedding_bert.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Treinamento do modelo Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classificar(X_train, X_test, Y_train, Y_test):\n",
    "    rfc = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rfc.fit(X_train, Y_train)\n",
    "    \n",
    "    # Fazendo previsões no conjunto de teste\n",
    "    y_pred = rfc.predict(X_test)\n",
    "\n",
    "    # Avaliando a acurácia\n",
    "    accuracy = accuracy_score(Y_test, y_pred)\n",
    "    print(f\"Acurácia: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Treinando com o formato TF\n",
      "Acurácia: 0.9425\n",
      "\n",
      "\n",
      "Treinando com o formato binário\n",
      "Acurácia: 0.9443\n",
      "\n",
      "\n",
      "Treinando com o formato TF-IDF\n",
      "Acurácia: 0.9408\n",
      "\n",
      "\n",
      "Treinando com word embeddings\n",
      "Acurácia: 0.9408\n",
      "\n",
      "\n",
      "Treinando com word embeddings pré-treinados\n",
      "Acurácia: 0.9282\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\nTreinando com o formato TF\")\n",
    "model_tf = classificar(X_train_tf, X_test_tf, y_train, y_test)\n",
    "\n",
    "print(\"\\n\\nTreinando com o formato binário\")\n",
    "model_bin = classificar(X_train_bin, X_test_bin, y_train, y_test)\n",
    "\n",
    "print(\"\\n\\nTreinando com o formato TF-IDF\")\n",
    "model_tfidf = classificar(X_train_tfidf, X_test_tfidf, y_train, y_test)\n",
    "\n",
    "print(\"\\n\\nTreinando com word embeddings\")\n",
    "model_embedding = classificar(X_train_embedding, X_test_embedding, y_train, y_test)\n",
    "\n",
    "print(\"\\n\\nTreinando com word embeddings pré-treinados\")\n",
    "model_embedding_bert = classificar(X_train_embedding_bert, X_test_embedding_bert, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Escolha dos Hiperparâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # Definindo os parâmetros para a busca em grade\n",
    "# param_grid = {\n",
    "#     'n_estimators': [100, 200, 300],\n",
    "#     'max_depth': [None, 10, 20, 30],\n",
    "#     'min_samples_split': [2, 5, 10],\n",
    "#     'min_samples_leaf': [1, 2, 4],\n",
    "#     'bootstrap': [True, False]\n",
    "# }\n",
    "\n",
    "# # Inicializando o RandomForestClassifier\n",
    "# clf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# # Inicializando o GridSearchCV\n",
    "# grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "\n",
    "# # Ajustando o modelo nos dados de treinamento\n",
    "# grid_search.fit(X_train_embedding_bert, y_train)\n",
    "\n",
    "# # Obtendo os melhores parâmetros\n",
    "# best_params = grid_search.best_params_\n",
    "# print(\"Melhores parâmetros:\", best_params)\n",
    "\n",
    "# # Treinando o modelo com os melhores parâmetros\n",
    "# best_clf = RandomForestClassifier(**best_params, random_state=42)\n",
    "# best_clf.fit(X_train_embedding_bert, y_train)\n",
    "\n",
    "# # Avaliando o modelo\n",
    "# y_pred = best_clf.predict(X_test_embedding_bert)\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(f\"Acurácia com os melhores parâmetros: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
